{
    "Linear Chain-of-Thought,0": {
        "thought": "By encouraging the LLM to think step by step rather than directly outputting an answer, chain-of-thought reasoning enables complex problem-solving through intermediate steps. This practice improves the model's ability to handle tasks that require deeper reasoning and provides insight into its decision-making process.",
        "name": "Chain-of-Thought",
        "code": "def forward(self, taskInfo):\n    # Instruction for the Chain-of-Thought (CoT) approach\n    # It is an important practice that allows the LLM to think step by step before solving the task.\n    cot_instruction = \"Please think step by step and then solve the task.\"\n\n    # Instantiate a new LLM agent specifically for CoT\n    # To allow LLM thinking before answering, we need to set an additional output field 'thinking'.\n    cot_agent = LLMAgentBase(['thinking', 'answer'], 'Chain-of-Thought Agent')\n\n    # Prepare the inputs for the CoT agent\n    # The input should be a list of Info, and the first one is often the taskInfo\n    cot_agent_inputs = [taskInfo]\n\n    # Get the response from the CoT agent\n    thinking, answer = cot_agent(cot_agent_inputs, cot_instruction)\n\n    # Return only the final answer\n    return answer\n",
        "api_calls": 1,
        "structure_label": "Linear Chain-of-Thought",
        "generation": "initial",
        "fitness": "95% Bootstrap Confidence Interval: (64.2%, 68.4%), Median: 77.0%"
    },
    "Linear Chain-of-Thought,1": null,
    "Iterative Refinement,0": {
        "thought": "**Insights:**\nTo enhance the agent's performance, we should employ distinct agent roles for different tasks. This would allow for clearer delineation of responsibilities and better specialization. An Evaluator agent could provide a structured way to assess feedback and decide whether to iterate or finalize the answer. \n\n**Overall Idea:**\nWe will implement a more dynamic architecture where the initial response comes from the main agent, feedback is evaluated by a dedicated Evaluator agent, and the refined answer is then produced by a separate Refiner agent. This will create a separation of concerns, allowing each agent to focus on its specific role. \n\n**Implementation:**\n1. Define the Main agent to generate the initial answer based on task information.  \n2. Use a dedicated Evaluator agent to assess the initial answer and feedback.  \n3. Establish a Refiner agent to create the final answer based on the feedback received from the Evaluator.  \n4. The loop will ensure that we can iterate until we achieve a satisfactory answer or reach a certain number of iterations.",
        "name": "Multi-Agent Iterative Refinement",
        "code": "def forward(self, taskInfo):\n    # Initial reasoning\n    initial_instruction = \"Analyze the provided information and formulate an answer.\"\n    main_agent = LLMAgentBase([\"thinking\", \"answer\"], \"Main Agent\", role=\"Helpful Assistant\")\n\n    # First API call - Generate initial answer\n    initial_thinking, initial_answer = main_agent([taskInfo], initial_instruction)\n\n    # Feedback evaluation and refinement in one call\n    feedback_instruction = \"Evaluate this answer and suggest improvements: {}. Using the feedback, improve your answer.\".format(initial_answer.content)\n\n    # Second API call - Get feedback and refine answer from the same agent\n    final_thinking, final_answer = main_agent([taskInfo, initial_answer], feedback_instruction)\n\n    # Return the final refined answer\n    return final_answer",
        "fitness": "95% Bootstrap Confidence Interval: (57.8%, 62.6%), Median: 71.8%",
        "generation": 2,
        "api_calls": 2,
        "structure_label": "Iterative Refinement"
    },
    "Iterative Refinement,1": {
        "thought": "To enhance its performance, an LLM can iteratively improve its answer based on feedback. By reflecting on its previous attempts and incorporating feedback, the model can refine its reasoning and provide a more accurate solution.",
        "name": "Self-Refine (Reflexion)",
        "code": "def forward(self, taskInfo):\n    # Instruction for initial reasoning\n    cot_initial_instruction = \"Please think step by step and then solve the task.\"\n\n    # Instruction for reflecting on previous attempts and feedback to improve\n    cot_reflect_instruction = \"Given previous attempts and feedback, carefully consider where you could go wrong in your latest attempt. Using insights from previous attempts, try to solve the task better.\"\n    cot_agent = LLMAgentBase(['thinking', 'answer'], 'Chain-of-Thought Agent')\n\n    # Instruction for providing feedback and correcting the answer\n    critic_instruction = \"Please review the answer above and criticize on where might be wrong. If you are absolutely sure it is correct, output 'True' in 'correct'.\"\n    critic_agent = LLMAgentBase(['feedback', 'correct'], 'Critic Agent')\n    \n    N_max = 5 # Maximum number of attempts\n\n    # Initial attempt\n    cot_inputs = [taskInfo]\n    thinking, answer = cot_agent(cot_inputs, cot_initial_instruction, 0)\n\n    for i in range(N_max):\n        # Get feedback and correct status from the critic\n        feedback, correct = critic_agent([taskInfo, thinking, answer], critic_instruction, i)\n        if correct.content == 'True':\n            break\n            \n        # Add feedback to the inputs for the next iteration\n        cot_inputs.extend([thinking, answer, feedback])\n\n        # Reflect on previous attempts and refine the answer\n        thinking, answer = cot_agent(cot_inputs, cot_reflect_instruction, i + 1)\n    return answer\n",
        "api_calls": 10,
        "structure_label": "Iterative Refinement",
        "generation": "initial",
        "fitness": "95% Bootstrap Confidence Interval: (58.4%, 63.0%), Median: 72.1%"
    },
    "Tree-of-Thought,0": null,
    "Tree-of-Thought,1": null,
    "Decompositional Reasoning,0": {
        "thought": "**Insights:**\nTo enhance the agent's reasoning capabilities, we can adopt a branching architecture that allows for multiple specialized paths based on the complexities of the task. By utilizing diverse agents for different reasoning aspects, we can improve the depth and accuracy of the answers generated. This architecture will also maintain efficiency in API calls while maximizing the overall effectiveness of the reasoning process.\n**Overall Idea:**\nThe new architecture will include multiple specialized agents that handle different aspects of the reasoning task. The task will be analyzed to determine which agent is best suited, and their outputs will be evaluated to select the optimal response. This dynamic approach enhances the responsiveness and adaptability of the agent.\n**Implementation:**\n1. Define multiple specialized agents capable of addressing distinct reasoning facets\u2014such as reading comprehension and logical deduction.\n2. Create a routing mechanism that analyzes the task and identifies the most relevant agents to engage for a particular query.\n3. Implement conditional branching to ensure that the selected agents process the task and return their results, which are then combined to form the final answer.",
        "name": "Dynamic Path Selection Agent",
        "code": "def forward(self, taskInfo):\n    # Define instruction for selecting the appropriate reasoning path\n    selection_instruction = \"Analyze the task and determine which specialized agent to use for answering the question.\"\n    agents = [\"Reading Comprehension Agent\", \"Logical Reasoning Agent\", \"General Knowledge Agent\"]  # Agent roles\n\n    # Routing the task to the appropriate expert\n    router = LLMAgentBase([\"choice\"], \"Routing agent\")\n    selected_path = router([taskInfo], selection_instruction)  # Call 1\n\n    # Extracting the content from the selected path\n    selected_content = selected_path[0].content  # Access the first element's content\n\n    # Use a single agent to handle the reasoning based on the selected path\n    main_agent = LLMAgentBase([\"thinking\", \"answer\"], \"Multi-Purpose Agent\")\n    refined_response = main_agent([taskInfo, selected_content], \"Generate answer using the selected agent role.\")  # Call 2\n\n    # Return the final refined answer\n    return refined_response[1]",
        "fitness": "95% Bootstrap Confidence Interval: (53.7%, 58.3%), Median: 67.9%",
        "generation": 4,
        "api_calls": 2,
        "structure_label": "Decompositional Reasoning"
    },
    "Decompositional Reasoning,1": {
        "thought": "Similar to Auto-GPT and expert prompting, we can use dynamic control flow in the design to let the agent decide what expert we should use.",
        "name": "Dynamic Assignment of Roles",
        "code": "def forward(self, taskInfo):\n        # Instruction for step-by-step reasoning\n        cot_instruction = \"Please think step by step and then solve the task.\"\n        expert_agents = [LLMAgentBase(['thinking', 'answer'], 'Expert Agent', role=role) for role in ['Reading Comprehension Specialist', 'Logical Reasoning Strategist', 'Multidisciplinary Knowledge Integrator', 'Helpful Assistant']]\n\n        # Instruction for routing the task to the appropriate expert\n        routing_instruction = \"Given the task, please choose an Expert to answer the question. Choose from: Reading Comprehension Specialist, Logical Reasoning Strategist, and Multidisciplinary Knowledge Integrator.\"\n        routing_agent = LLMAgentBase(['choice'], 'Routing agent')\n\n        # Get the choice of expert to route the task\n        choice = routing_agent([taskInfo], routing_instruction)[0]\n\n        if 'specialist' in choice.content.lower():\n            expert_id = 0\n        elif 'strategist' in choice.content.lower():\n            expert_id = 1\n        elif 'integrator' in choice.content.lower():\n            expert_id = 2\n        else:\n            expert_id = 3 # Default to helpful assistant\n\n        thinking, answer = expert_agents[expert_id]([taskInfo], cot_instruction)\n        return answer\n",
        "api_calls": 6,
        "structure_label": "Decompositional Reasoning",
        "generation": "initial",
        "fitness": "95% Bootstrap Confidence Interval: (65.8%, 70.3%), Median: 78.7%"
    },
    "Multi-Agent Reasoning,0": {
        "thought": "**Insights:**\nTo enhance the reasoning capabilities of the agent while maintaining a Tree-of-Thought structure, we can implement a filtering mechanism that evaluates the outputs of each specialized agent prior to synthesis. This would ensure that the synthesis agent combines only the most relevant insights, leading to a more accurate answer.\n\n**Overall Idea:**\nIncorporate a scoring mechanism to rank the outputs from the reasoning agents. The synthesis agent will then combine only the highest-ranked outputs, enhancing the clarity and reliability of the final answer while preserving the multi-agent approach.\n\n**Implementation:**\n1. Define multiple agents for distinct reasoning paths, focusing on comprehension, numerical insights, and logical deductions.\n2. Each agent will provide insights based on the same task input.\n3. Integrate scoring and synthesis into a single step to reduce API calls while maintaining relevance and clarity.",
        "name": "Ranked Insight Synthesis Agent",
        "code": "def forward(self, taskInfo):\n    # Instruction for the Parallel Reasoning approach\n    instruction = \"Analyze the passage from different perspectives to derive a comprehensive answer.\"\n\n    # Create multiple agents for diverse reasoning tasks\n    comprehension_agent = LLMAgentBase([\"thinking\", \"answer\"], \"Comprehension Agent\")  # Analyzes the main ideas\n    numerical_agent = LLMAgentBase([\"thinking\", \"answer\"], \"Numerical Analysis Agent\")  # Focuses on population numbers\n    logical_agent = LLMAgentBase([\"thinking\", \"answer\"], \"Logical Deduction Agent\")  # Engages in reasoning about implications\n\n    # Prepare inputs for each agent\n    inputs = [taskInfo]  # Common input for all agents\n\n    # Call each reasoning agent to gather insights\n    comprehension_info = comprehension_agent(inputs, instruction)  # Call 1\n    numerical_info = numerical_agent(inputs, instruction)  # Call 2\n    logical_info = logical_agent(inputs, instruction)  # Call 3\n\n    # Collect answers from all agents\n    answers = [comprehension_info[1], numerical_info[1], logical_info[1]]  # Extracting answers from Info objects\n\n    # Combine scoring and synthesis into one step\n    synthesis_agent = LLMAgentBase([\"thinking\", \"final_answer\"], \"Synthesis Agent\")\n    final_info = synthesis_agent(answers, \"Evaluate and combine the insights to provide the best answer.\")  # Call 4\n\n    # Return the final synthesized answer\n    return final_info[1]",
        "fitness": "95% Bootstrap Confidence Interval: (54.9%, 59.7%), Median: 69.2%",
        "generation": 6,
        "api_calls": 4,
        "structure_label": "Multi-Agent Reasoning"
    },
    "Multi-Agent Reasoning,1": {
        "thought": "By letting different LLMs debate with each other, we can leverage their diverse perspectives to find better solutions for tasks.",
        "name": "LLM Debate",
        "code": "def forward(self, taskInfo):\n    # Instruction for initial reasoning\n    debate_initial_instruction = \"Please think step by step and then solve the task.\"\n\n    # Instruction for debating and updating the solution based on other agents' solutions\n    debate_instruction = \"Given solutions to the problem from other agents, consider their opinions as additional advice. Please think carefully and provide an updated answer.\"\n    \n    # Initialize debate agents with different roles and a moderate temperature for varied reasoning\n    debate_agents = [LLMAgentBase(['thinking', 'answer'], 'Debate Agent', temperature=0.8, role=role) for role in ['Reading Comprehension Specialist', 'Logical Reasoning Strategist', 'Multidisciplinary Knowledge Integrator']]\n\n    # Instruction for final decision-making based on all debates and solutions\n    final_decision_instruction = \"Given all the above thinking and answers, reason over them carefully and provide a final answer.\"\n    final_decision_agent = LLMAgentBase(['thinking', 'answer'], 'Final Decision Agent', temperature=0.1)\n\n    max_round = 2 # Maximum number of debate rounds\n    all_thinking = [[] for _ in range(max_round)]\n    all_answer = [[] for _ in range(max_round)]\n\n    # Perform debate rounds\n    for r in range(max_round):\n        for i in range(len(debate_agents)):\n            if r == 0:\n                thinking, answer = debate_agents[i]([taskInfo], debate_initial_instruction)\n            else:\n                input_infos = [taskInfo] + [all_thinking[r-1][i]] + all_thinking[r-1][:i] + all_thinking[r-1][i+1:]\n                thinking, answer = debate_agents[i](input_infos, debate_instruction)\n            all_thinking[r].append(thinking)\n            all_answer[r].append(answer)\n    \n    # Make the final decision based on all debate results and solutions\n    thinking, answer = final_decision_agent([taskInfo] + all_thinking[max_round-1] + all_answer[max_round-1], final_decision_instruction)\n    return answer\n",
        "api_calls": 12,
        "structure_label": "Multi-Agent Reasoning",
        "generation": "initial",
        "fitness": "95% Bootstrap Confidence Interval: (34.5%, 39.5%), Median: 49.7%"
    },
    "Abstraction to Principles Reasoning,0": {
        "thought": "Let LLM first think about the principles involved in solving this task which could be helpful. By understanding the underlying principles, the model can better reason through the problem and provide a more accurate solution.",
        "name": "Step-back Abstraction",
        "code": "def forward(self, taskInfo):\n        # Instruction for understanding the principles involved in the task\n        principle_instruction = \"What are the physics, chemistry or biology principles and concepts involved in solving this task? First think step by step. Then list all involved principles and explain them.\"\n        \n        # Instruction for solving the task based on the principles\n        cot_instruction = \"Given the question and the involved principle behind the question, think step by step and then solve the task.\"\n        \n        # Instantiate LLM agents\n        principle_agent = LLMAgentBase(['thinking', 'principle'], 'Principle Agent')\n        cot_agent = LLMAgentBase(['thinking', 'answer'], 'Chain-of-Thought Agent')\n        \n        # Get the principles involved in the task\n        thinking, principle = principle_agent([taskInfo], principle_instruction)\n\n        # Use the principles to solve the task\n        thinking, answer = cot_agent([taskInfo, thinking, principle], cot_instruction)\n        return answer\n",
        "api_calls": 4,
        "structure_label": "Abstraction to Principles Reasoning",
        "generation": "initial",
        "fitness": "95% Bootstrap Confidence Interval: (63.9%, 67.8%), Median: 76.3%"
    },
    "Abstraction to Principles Reasoning,1": null
}