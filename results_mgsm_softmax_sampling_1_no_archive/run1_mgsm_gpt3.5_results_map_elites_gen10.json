{
    "Linear Chain-of-Thought,0": {
        "thought": "By encouraging the LLM to think step by step rather than directly outputting an answer, chain-of-thought reasoning enables complex problem-solving through intermediate steps. This practice improves the model's ability to handle tasks that require deeper reasoning and provides insight into its decision-making process.",
        "name": "Chain-of-Thought",
        "code": "def forward(self, taskInfo):\n    # Instruction for the Chain-of-Thought (CoT) approach\n    # It is an important practice that allows the LLM to think step by step before solving the task.\n    cot_instruction = \"Please think step by step and then solve the task.\"\n\n    # Instantiate a new LLM agent specifically for CoT\n    # To allow LLM thinking before answering, we need to set an additional output field 'thinking'.\n    cot_agent = LLMAgentBase(['thinking', 'answer'], 'Chain-of-Thought Agent')\n\n    # Prepare the inputs for the CoT agent\n    # The input should be a list of Info, and the first one is often the taskInfo\n    cot_agent_inputs = [taskInfo]\n\n    # Get the response from the CoT agent\n    thinking, answer = cot_agent(cot_agent_inputs, cot_instruction)\n\n    # Return only the final answer\n    return answer\n",
        "api_calls": 1,
        "structure_label": "Linear Chain-of-Thought",
        "generation": "initial",
        "fitness": "95% Bootstrap Confidence Interval: (7.0%, 18.8%), Median: 12.5%"
    },
    "Linear Chain-of-Thought,1": null,
    "Iterative Refinement,0": null,
    "Iterative Refinement,1": {
        "thought": "To enhance its performance, an LLM can iteratively improve its answer based on feedback. By reflecting on its previous attempts and incorporating feedback, the model can refine its reasoning and provide a more accurate solution.",
        "name": "Self-Refine (Reflexion)",
        "code": "def forward(self, taskInfo):\n    # Instruction for initial reasoning\n    cot_initial_instruction = \"Please think step by step and then solve the task.\"\n\n    # Instruction for reflecting on previous attempts and feedback to improve\n    cot_reflect_instruction = \"Given previous attempts and feedback, carefully consider where you could go wrong in your latest attempt. Using insights from previous attempts, try to solve the task better.\"\n    cot_agent = LLMAgentBase(['thinking', 'answer'], 'Chain-of-Thought Agent')\n\n    # Instruction for providing feedback and correcting the answer\n    critic_instruction = \"Please review the answer above and criticize on where might be wrong. If you are absolutely sure it is correct, output 'True' in 'correct'.\"\n    critic_agent = LLMAgentBase(['feedback', 'correct'], 'Critic Agent')\n    \n    N_max = 5 # Maximum number of attempts\n\n    # Initial attempt\n    cot_inputs = [taskInfo]\n    thinking, answer = cot_agent(cot_inputs, cot_initial_instruction, 0)\n\n    for i in range(N_max):\n        # Get feedback and correct status from the critic\n        feedback, correct = critic_agent([taskInfo, thinking, answer], critic_instruction, i)\n        if correct.content == 'True':\n            break\n            \n        # Add feedback to the inputs for the next iteration\n        cot_inputs.extend([thinking, answer, feedback])\n\n        # Reflect on previous attempts and refine the answer\n        thinking, answer = cot_agent(cot_inputs, cot_reflect_instruction, i + 1)\n    return answer\n",
        "api_calls": 10,
        "structure_label": "Iterative Refinement",
        "generation": "initial",
        "fitness": "95% Bootstrap Confidence Interval: (10.2%, 23.4%), Median: 16.4%"
    },
    "Tree-of-Thought,0": {
        "thought": "**Insights:**\nTo enhance the mathematical problem-solving agent, it is vital to explore a structure that not only separates reasoning and validation but also integrates a tree-based approach to allow for multiple reasoning pathways. This would facilitate a broader exploration of potential solutions and enhance the consensus process. \n**Overall Idea:**\nI propose an architecture that utilizes a tree-of-thought mechanism, where the reasoning process branches into multiple paths based on different interpretations of the problem. Each branch will be resolved by a dedicated agent that focuses on a specific aspect of the problem, and the results will converge to reach a final answer through a consensus mechanism. \n**Implementation:**\n1. Implement a reasoning agent that explores various potential solutions based on different approaches.\n2. For all potential solutions, run a single validation agent that assesses the accuracy of those solutions.\n3. Converge the results from the validation agents to determine the most plausible final answer.",
        "name": "Tree-of-Thought Reasoning Architecture",
        "code": "def forward(self, taskInfo):\n    # Instruction for the reasoning agent to explore multiple pathways\n    reasoning_instruction = \"Break down the mathematical problem into various approaches and propose possible solutions for each.\"\n    reasoning_agent = LLMAgentBase([\"thinking\", \"solutions\"], \"Multi-Path Reasoning Agent\")\n    potential_solutions = reasoning_agent([taskInfo], reasoning_instruction)  # 1 call\n\n    # Instructions for validation of all proposed solutions\n    validation_instruction = \"Evaluate the set of proposed solutions for correctness.\"\n    validation_agent = LLMAgentBase([\"thinking\", \"validation_results\"], \"Validation Agent\")\n    validation_response = validation_agent([taskInfo, potential_solutions[1]], validation_instruction)  # 1 call for all solutions\n\n    # Determine the most plausible answer based on validation results\n    consensus_instruction = \"Analyze the validation results and select the most reliable solution.\"\n    consensus_agent = LLMAgentBase([\"thinking\", \"final_answer\"], \"Consensus Agent\")\n    final_response = consensus_agent([taskInfo, validation_response[1]], consensus_instruction)  # 1 call\n\n    return final_response[1]  # Return the final answer.",
        "fitness": "95% Bootstrap Confidence Interval: (54.7%, 71.9%), Median: 63.3%",
        "generation": 10,
        "api_calls": 3,
        "structure_label": "Tree-of-Thought"
    },
    "Tree-of-Thought,1": null,
    "Decompositional Reasoning,0": {
        "thought": "**Insights:**\nTo enhance the performance of the mathematical problem-solving agent, a clear delineation between reasoning, validation, and consensus processes is essential. Each component should have specific instructions to guide their roles effectively. \n**Overall Idea:**\nI propose an architecture that utilizes three specialized agents: one for reasoning, one for validation, and one for consensus decision-making. This structure ensures that each part of the process is optimized for its function while allowing for better feedback integration. \n**Implementation:**\n1. Implement a reasoning agent that breaks down the problem and proposes an initial solution.\n2. Introduce a validation agent that evaluates the proposed solution against set criteria to ensure its accuracy.\n3. Finally, use a consensus agent to bring together feedback and arrive at the final answer, ensuring that prior solutions are weighed effectively against the criteria.",
        "name": "Decomposed Reasoning with Feedback Integration Architecture",
        "code": "def forward(self, taskInfo):\n    # Instruction for the reasoning agent\n    reasoning_instruction = \"Analyze the mathematical problem and break it down into clear components while proposing an initial solution.\"\n    reasoning_agent = LLMAgentBase([\"thinking\", \"intermediate_solution\"], \"Reasoning Agent\")\n    intermediate_response = reasoning_agent([taskInfo], reasoning_instruction)  # 1 call\n\n    # Instruction for the validation and consensus combined agent\n    combined_instruction = \"Evaluate the intermediate solution for correctness and provide critical feedback, then determine the final answer based on this evaluation.\"\n    combined_agent = LLMAgentBase([\"thinking\", \"final_answer\"], \"Validation and Consensus Agent\")\n    final_response = combined_agent([taskInfo, intermediate_response[1]], combined_instruction)  # 1 call\n\n    return final_response[1]  # Return the final answer.",
        "fitness": "95% Bootstrap Confidence Interval: (55.5%, 72.7%), Median: 64.1%",
        "generation": 8,
        "api_calls": 2,
        "structure_label": "Decompositional Reasoning"
    },
    "Decompositional Reasoning,1": null,
    "Multi-Agent Reasoning,0": {
        "thought": "**Insights:**\nThe architecture allows for diverse reasoning through independent agents, but can benefit from a clearer delineation of agent roles. By using specialized agents to tackle distinct aspects of the problem, we can improve the final consensus through more focused contributions. \n**Overall Idea:**\nI propose a multi-agent architecture where each agent is responsible for a specific sub-task in solving the mathematical problem, with a final agent dedicated to resolving any conflicts among outputs. This structure will enhance the diversity of solutions and optimize the consensus process. \n**Implementation:**\n1. Define three specialized agents: one for initial reasoning, another for generating alternatives, and a third for reaching consensus.\n2. Each agent will receive tailored instructions to maximize their effectiveness in solving the task.\n3. The consensus agent will aggregate results from the first two agents and determine the most viable solution based on their outputs.",
        "name": "Specialized Multi-Agent Consensus Architecture",
        "code": "def forward(self, taskInfo):\n    # Define instructions for specialized reasoning\n    initial_instruction = \"Analyze the task and provide a solution step by step.\"\n    alternative_instruction = \"Based on the first solution, suggest alternative methods to solve the task.\"\n    consensus_instruction = \"Review the solutions and select the best one based on consensus.\"\n\n    # Instantiate one agent for all tasks\n    agent = LLMAgentBase([\"thinking\", \"answer\"], \"Multi-Task Agent\")\n\n    # Step 1: Get initial reasoning\n    initial_thinking, initial_answer = agent([taskInfo], initial_instruction)\n\n    # Step 2: Generate alternatives based on the initial answer\n    alternative_thinking, alternative_answer = agent([taskInfo, initial_answer], alternative_instruction)\n\n    # Step 3: Prepare inputs for consensus\n    responses = [initial_answer, alternative_answer]\n\n    # Step 4: Consensus to determine the best answer\n    consensus_thinking, final_answer = agent([taskInfo] + responses, consensus_instruction)\n\n    return final_answer",
        "fitness": "95% Bootstrap Confidence Interval: (50.0%, 67.2%), Median: 58.6%",
        "generation": 2,
        "api_calls": 3,
        "structure_label": "Multi-Agent Reasoning"
    },
    "Multi-Agent Reasoning,1": {
        "thought": "Similar to Quality-Diversity methods, let LLM generate multiple diverse interesting solutions could help. By encouraging the model to explore different reasoning paths, we can increase the chances of finding the best solution.",
        "name": "Quality-Diversity",
        "code": "def forward(self, taskInfo):\n    # Instruction for initial reasoning\n    cot_initial_instruction = \"Please think step by step and then solve the task.\"\n\n    # Instruction for giving diverse answers\n    qd_instruction = \"Given previous attempts, try to come up with another interesting way to solve the task.\"\n    cot_agent = LLMAgentBase(['thinking', 'answer'], 'Chain-of-Thought Agent')\n\n    # Instruction for final decision-making based on collected reasoning and answers\n    final_decision_instruction = \"Given all the above solutions, reason over them carefully and provide a final answer.\"\n    final_decision_agent = LLMAgentBase(['thinking', 'answer'], 'Final Decision Agent', temperature=0.1)\n    \n    N_max = 3 # Maximum number of attempts\n\n    # Initial attempt\n    cot_inputs = [taskInfo]\n    possible_answers = []\n    thinking, answer = cot_agent(cot_inputs, cot_initial_instruction, 0)\n\n    # Add the answer to the list of possible answers\n    possible_answers.extend([thinking, answer])\n\n    for i in range(N_max):\n        # Reflect on previous attempts and generate another interesting answer\n        cot_inputs.extend([thinking, answer])\n\n        # Generate another interesting answer\n        thinking, answer = cot_agent(cot_inputs, qd_instruction, i + 1)\n        possible_answers.extend([thinking, answer])\n\n    # Make the final decision based on all generated answers\n    thinking, answer = final_decision_agent([taskInfo] + possible_answers, final_decision_instruction)\n    return answer\n",
        "api_calls": 8,
        "structure_label": "Multi-Agent Reasoning",
        "generation": "initial",
        "fitness": "95% Bootstrap Confidence Interval: (49.2%, 66.4%), Median: 57.8%"
    },
    "Abstraction to Principles Reasoning,0": {
        "thought": "Let LLM first think about the principles involved in solving this task which could be helpful. By understanding the underlying principles, the model can better reason through the problem and provide a more accurate solution.",
        "name": "Step-back Abstraction",
        "code": "def forward(self, taskInfo):\n        # Instruction for understanding the principles involved in the task\n        principle_instruction = \"What are the physics, chemistry or biology principles and concepts involved in solving this task? First think step by step. Then list all involved principles and explain them.\"\n        \n        # Instruction for solving the task based on the principles\n        cot_instruction = \"Given the question and the involved principle behind the question, think step by step and then solve the task.\"\n        \n        # Instantiate LLM agents\n        principle_agent = LLMAgentBase(['thinking', 'principle'], 'Principle Agent')\n        cot_agent = LLMAgentBase(['thinking', 'answer'], 'Chain-of-Thought Agent')\n        \n        # Get the principles involved in the task\n        thinking, principle = principle_agent([taskInfo], principle_instruction)\n\n        # Use the principles to solve the task\n        thinking, answer = cot_agent([taskInfo, thinking, principle], cot_instruction)\n        return answer\n",
        "api_calls": 4,
        "structure_label": "Abstraction to Principles Reasoning",
        "generation": "initial",
        "fitness": "95% Bootstrap Confidence Interval: (18.8%, 34.4%), Median: 26.6%"
    },
    "Abstraction to Principles Reasoning,1": null
}