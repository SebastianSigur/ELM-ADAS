{
    "Abstraction to Principles Reasoning,0": {
        "thought": "Let LLM first think about the principles involved in solving this task which could be helpful. By understanding the underlying principles, the model can better reason through the problem and provide a more accurate solution.",
        "name": "Step-back Abstraction",
        "code": "def forward(self, taskInfo):\n        # Instruction for understanding the principles involved in the task\n        principle_instruction = \"What are the physics, chemistry or biology principles and concepts involved in solving this task? First think step by step. Then list all involved principles and explain them.\"\n        \n        # Instruction for solving the task based on the principles\n        cot_instruction = \"Given the question and the involved principle behind the question, think step by step and then solve the task.\"\n        \n        # Instantiate LLM agents\n        principle_agent = LLMAgentBase(['thinking', 'principle'], 'Principle Agent')\n        cot_agent = LLMAgentBase(['thinking', 'answer'], 'Chain-of-Thought Agent')\n        \n        # Get the principles involved in the task\n        thinking, principle = principle_agent([taskInfo], principle_instruction)\n\n        # Use the principles to solve the task\n        thinking, answer = cot_agent([taskInfo, thinking, principle], cot_instruction)\n        return answer\n",
        "api_calls": 2,
        "structure_label": "Abstraction to Principles Reasoning",
        "generation": "initial",
        "fitness": "95% Bootstrap Confidence Interval: (13.3%, 27.3%), Median: 20.3%"
    },
    "Abstraction to Principles Reasoning,1": null,
    "Chain-of-Thought Reasoning,0": {
        "thought": "By encouraging the LLM to think step by step rather than directly outputting an answer, chain-of-thought reasoning enables complex problem-solving through intermediate steps. This practice improves the model's ability to handle tasks that require deeper reasoning and provides insight into its decision-making process.",
        "name": "Chain-of-Thought",
        "code": "def forward(self, taskInfo):\n    # Instruction for the Chain-of-Thought (CoT) approach\n    # It is an important practice that allows the LLM to think step by step before solving the task.\n    cot_instruction = \"Please think step by step and then solve the task.\"\n\n    # Instantiate a new LLM agent specifically for CoT\n    # To allow LLM thinking before answering, we need to set an additional output field 'thinking'.\n    cot_agent = LLMAgentBase(['thinking', 'answer'], 'Chain-of-Thought Agent')\n\n    # Prepare the inputs for the CoT agent\n    # The input should be a list of Info, and the first one is often the taskInfo\n    cot_agent_inputs = [taskInfo]\n\n    # Get the response from the CoT agent\n    thinking, answer = cot_agent(cot_agent_inputs, cot_instruction)\n\n    # Return only the final answer\n    return answer\n",
        "api_calls": 1,
        "structure_label": "Chain-of-Thought Reasoning",
        "generation": "initial",
        "fitness": "95% Bootstrap Confidence Interval: (7.8%, 19.5%), Median: 13.3%"
    },
    "Chain-of-Thought Reasoning,1": null,
    "Multi-Agent Reasoning,0": {
        "thought": "Similar to Auto-GPT and expert prompting, we can use dynamic control flow in the design to let the agent decide what expert we should use.",
        "name": "Dynamic Assignment of Roles",
        "code": "def forward(self, taskInfo):\n        # Instruction for step-by-step reasoning\n        cot_instruction = \"Please think step by step and then solve the task.\"\n        expert_agents = [LLMAgentBase(['thinking', 'answer'], 'Expert Agent', role=role) for role in ['Math Professor', 'Grade School Teacher', 'Math Enthusiast', 'Helpful Assistant']]\n\n        # Instruction for routing the task to the appropriate expert\n        routing_instruction = \"Given the task, please choose an Expert to answer the question. Choose from: Math Professor, Grade School Teacher, Math Enthusiast.\"\n        routing_agent = LLMAgentBase(['choice'], 'Routing agent')\n\n        # Get the choice of expert to route the task\n        choice = routing_agent([taskInfo], routing_instruction)[0]\n\n        if 'professor' in choice.content.lower():\n            expert_id = 0\n        elif 'teacher' in choice.content.lower():\n            expert_id = 1\n        elif 'enthusiast' in choice.content.lower():\n            expert_id = 2\n        else:\n            expert_id = 3 # Default to helpful assistant\n\n        thinking, answer = expert_agents[expert_id]([taskInfo], cot_instruction)\n        return answer\n",
        "api_calls": 4,
        "structure_label": "Multi-Agent Reasoning",
        "generation": "initial",
        "fitness": "95% Bootstrap Confidence Interval: (6.2%, 17.2%), Median: 11.7%"
    },
    "Multi-Agent Reasoning,1": {
        "thought": "**Insights:**\nIncorporating a feedback loop where the consensus agent provides insights back to the reasoning agents allows for iterative improvement of their outputs. This will enhance the overall decision-making process by enabling the agents to refine their answers based on collective reasoning. Each agent will first generate an answer, then receive feedback, and have a chance to improve their solution. \n\n**Overall Idea:**\nThe updated architecture will involve a two-phase process where reasoning agents first generate their responses, then a consensus agent evaluates these responses and provides feedback that allows the reasoning agents to refine their answers before coming to a final consensus. \n\n**Implementation Steps:**\n1. Initialize reasoning agents to generate independent answers.\n2. Have a consensus agent evaluate these answers and provide feedback.\n3. Allow reasoning agents to revise their outputs based on the feedback.\n4. Finally, synthesize the refined answers to reach a consensus.",
        "name": "Iterative Consensus Refinement",
        "code": "def forward(self, taskInfo):\n    # Instruction for independent reasoning from multiple agents\n    independent_instruction = \"Please think step by step and solve the task independently.\"\n\n    # Initialize three reasoning agents to provide diverse insights\n    reasoning_agents = [LLMAgentBase(['thinking', 'answer'], 'Reasoning Agent', role=role) for role in ['Math Professor', 'Grade School Teacher', 'Math Enthusiast']]\n\n    # Prepare a list to collect responses\n    all_responses = []\n    for agent in reasoning_agents:\n        response = agent([taskInfo], independent_instruction)\n        all_responses.append(response)\n\n    # Gather the thinking and answers from all responses\n    all_thinking = [response[0] for response in all_responses]\n    all_answers = [response[1] for response in all_responses]\n\n    # Instruction for the consensus agent to evaluate and aggregate thoughts and answers\n    consensus_instruction = \"Based on the reasoning and answers provided, come to a consensus on the final answer to the task and suggest feedback for the reasoning agents.\"\n    consensus_agent = LLMAgentBase(['thinking', 'final_answer', 'feedback'], 'Consensus Agent')\n\n    # Aggregate the reasoning and answers for the consensus agent\n    combined_input = [taskInfo] + all_thinking + all_answers\n    consensus_thinking, final_answer, feedback = consensus_agent(combined_input, consensus_instruction)\n\n    # Allow reasoning agents to revise their outputs based on feedback\n    revised_responses = []\n    for agent in reasoning_agents:\n        response = agent([taskInfo, feedback], independent_instruction)\n        revised_responses.append(response)\n\n    # Final revised answers\n    final_revised_thinking = [response[0] for response in revised_responses]\n    final_revised_answers = [response[1] for response in revised_responses]\n\n    # Synthesize final answers using a consensus decision agent\n    final_decision_agent = LLMAgentBase(['thinking', 'final_decision'], 'Final Decision Agent')\n    final_combined_input = [taskInfo] + final_revised_thinking + final_revised_answers\n    final_thinking, final_decision = final_decision_agent(final_combined_input, \"Make a final decision based on the revised answers.\")\n\n    return final_decision",
        "fitness": "95% Bootstrap Confidence Interval: (67.2%, 82.0%), Median: 75.0%",
        "generation": 4,
        "api_calls": 7,
        "structure_label": "Multi-Agent Reasoning"
    },
    "Self-Reflection Reasoning,0": {
        "thought": "**Insights:** To optimize the agent's performance while adhering to the rules, I will design a single-step architecture that combines the generation of the answer and the critique into one cohesive process without needing subsequent API calls. This architecture will maintain the principle-focused approach while reducing complexity. \n\n**Overall Idea:** This design will prompt the agent to think through the principles involved, solve the task, and also critique its reasoning in one go. It aims to enhance the understanding of the principles while making the problem-solving more efficient. \n\n**Implementation:** The instruction will guide the LLM to think step-by-step, solve the task, and critique its own answer, all within a single response.",
        "name": "Principles and Self-Critique Integration",
        "code": "def forward(self, taskInfo):\n    # Single instruction for reasoning and self-reflection\n    instruction = \"Please think step by step to identify the principles related to solving this task. Then solve the task and provide a self-critique with suggestions for improvement in one response.\"\n    \n    # Instantiate a single LLM agent to handle the task\n    reflective_agent = LLMAgentBase(['thinking', 'answer', 'feedback'], 'Principles and Self-Critique Integration Agent')\n    \n    # Prepare inputs for the agent\n    inputs = [taskInfo]\n    \n    # Get the response which includes the initial thinking, answer, and feedback in one call\n    response = reflective_agent(inputs, instruction)\n    \n    # Return the answer directly from the response\n    return response[1]",
        "fitness": "95% Bootstrap Confidence Interval: (34.4%, 51.6%), Median: 43.0%",
        "generation": 7,
        "api_calls": 1,
        "structure_label": "Self-Reflection Reasoning"
    },
    "Self-Reflection Reasoning,1": {
        "thought": "To enhance its performance, an LLM can iteratively improve its answer based on feedback. By reflecting on its previous attempts and incorporating feedback, the model can refine its reasoning and provide a more accurate solution.",
        "name": "Self-Refine (Reflexion)",
        "code": "def forward(self, taskInfo):\n    # Instruction for initial reasoning\n    cot_initial_instruction = \"Please think step by step and then solve the task.\"\n\n    # Instruction for reflecting on previous attempts and feedback to improve\n    cot_reflect_instruction = \"Given previous attempts and feedback, carefully consider where you could go wrong in your latest attempt. Using insights from previous attempts, try to solve the task better.\"\n    cot_agent = LLMAgentBase(['thinking', 'answer'], 'Chain-of-Thought Agent')\n\n    # Instruction for providing feedback and correcting the answer\n    critic_instruction = \"Please review the answer above and criticize on where might be wrong. If you are absolutely sure it is correct, output 'True' in 'correct'.\"\n    critic_agent = LLMAgentBase(['feedback', 'correct'], 'Critic Agent')\n    \n    N_max = 5 # Maximum number of attempts\n\n    # Initial attempt\n    cot_inputs = [taskInfo]\n    thinking, answer = cot_agent(cot_inputs, cot_initial_instruction, 0)\n\n    for i in range(N_max):\n        # Get feedback and correct status from the critic\n        feedback, correct = critic_agent([taskInfo, thinking, answer], critic_instruction, i)\n        if correct.content == 'True':\n            break\n            \n        # Add feedback to the inputs for the next iteration\n        cot_inputs.extend([thinking, answer, feedback])\n\n        # Reflect on previous attempts and refine the answer\n        thinking, answer = cot_agent(cot_inputs, cot_reflect_instruction, i + 1)\n    return answer\n",
        "api_calls": 7,
        "structure_label": "Self-Reflection Reasoning",
        "generation": "initial",
        "fitness": "95% Bootstrap Confidence Interval: (10.9%, 24.2%), Median: 17.2%"
    }
}