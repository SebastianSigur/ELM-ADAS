[
    {
        "thought": "By encouraging the LLM to think step by step rather than directly outputting an answer, chain-of-thought reasoning enables complex problem-solving through intermediate steps. This practice improves the model's ability to handle tasks that require deeper reasoning and provides insight into its decision-making process.",
        "name": "Chain-of-Thought",
        "code": "def forward(self, taskInfo):\n    # Instruction for the Chain-of-Thought (CoT) approach\n    # It is an important practice that allows the LLM to think step by step before solving the task.\n    cot_instruction = \"Please think step by step and then solve the task.\"\n\n    # Instantiate a new LLM agent specifically for CoT\n    # To allow LLM thinking before answering, we need to set an additional output field 'thinking'.\n    cot_agent = LLMAgentBase(['thinking', 'answer'], 'Chain-of-Thought Agent')\n\n    # Prepare the inputs for the CoT agent\n    # The input should be a list of Info, and the first one is often the taskInfo\n    cot_agent_inputs = [taskInfo]\n\n    # Get the response from the CoT agent\n    thinking, answer = cot_agent(cot_agent_inputs, cot_instruction)\n\n    # Return only the final answer\n    return answer\n",
        "api_calls": 1,
        "structure_label": "Linear Chain-of-Thought",
        "generation": "initial",
        "fitness": "95% Bootstrap Confidence Interval: (8.6%, 20.3%), Median: 14.1%",
        "test_fitness": "95% Bootstrap Confidence Interval: (13.0%, 18.0%), Median: 15.5%"
    },
    {
        "thought": "While an LLM can arrive at the correct answer, its reasoning may vary. By repeatedly asking the same question with high temperature settings, we can generate different reasoning paths. We then combine multiple answers from these Chain-of-Thought (CoT) agents to produce a more accurate final answer through ensembling.",
        "name": "Self-Consistency with Chain-of-Thought",
        "code": "def forward(self, taskInfo):\n    # Instruction for step-by-step reasoning\n    cot_instruction = \"Please think step by step and then solve the task.\"\n    N = 5 # Number of CoT agents\n\n    # Initialize multiple CoT agents with a higher temperature for varied reasoning\n    cot_agents = [LLMAgentBase(['thinking', 'answer'], 'Chain-of-Thought Agent', temperature=0.8) for _ in range(N)]\n\n    # Majority voting function to select the most common answer\n    from collections import Counter\n    def majority_voting(answers):\n        return Counter(answers).most_common(1)[0][0]\n    \n    possible_answers = []\n    for i in range(N):\n        thinking, answer = cot_agents[i]([taskInfo], cot_instruction)\n        possible_answers.append(answer.content)\n\n    # Ensembling the answers from multiple CoT agents\n    answer = majority_voting(possible_answers)\n    return answer  \n",
        "api_calls": 5,
        "structure_label": "Multi-Agent Reasoning",
        "generation": "initial",
        "fitness": "95% Bootstrap Confidence Interval: (8.6%, 21.1%), Median: 14.8%",
        "test_fitness": "95% Bootstrap Confidence Interval: (10.2%, 14.9%), Median: 12.5%"
    },
    {
        "thought": "To enhance its performance, an LLM can iteratively improve its answer based on feedback. By reflecting on its previous attempts and incorporating feedback, the model can refine its reasoning and provide a more accurate solution.",
        "name": "Self-Refine (Reflexion)",
        "code": "def forward(self, taskInfo):\n    # Instruction for initial reasoning\n    cot_initial_instruction = \"Please think step by step and then solve the task.\"\n\n    # Instruction for reflecting on previous attempts and feedback to improve\n    cot_reflect_instruction = \"Given previous attempts and feedback, carefully consider where you could go wrong in your latest attempt. Using insights from previous attempts, try to solve the task better.\"\n    cot_agent = LLMAgentBase(['thinking', 'answer'], 'Chain-of-Thought Agent')\n\n    # Instruction for providing feedback and correcting the answer\n    critic_instruction = \"Please review the answer above and criticize on where might be wrong. If you are absolutely sure it is correct, output 'True' in 'correct'.\"\n    critic_agent = LLMAgentBase(['feedback', 'correct'], 'Critic Agent')\n    \n    N_max = 5 # Maximum number of attempts\n\n    # Initial attempt\n    cot_inputs = [taskInfo]\n    thinking, answer = cot_agent(cot_inputs, cot_initial_instruction, 0)\n\n    for i in range(N_max):\n        # Get feedback and correct status from the critic\n        feedback, correct = critic_agent([taskInfo, thinking, answer], critic_instruction, i)\n        if correct.content == 'True':\n            break\n            \n        # Add feedback to the inputs for the next iteration\n        cot_inputs.extend([thinking, answer, feedback])\n\n        # Reflect on previous attempts and refine the answer\n        thinking, answer = cot_agent(cot_inputs, cot_reflect_instruction, i + 1)\n    return answer\n",
        "api_calls": 10,
        "structure_label": "Iterative Refinement",
        "generation": "initial",
        "fitness": "95% Bootstrap Confidence Interval: (11.7%, 25.0%), Median: 18.0%",
        "test_fitness": "95% Bootstrap Confidence Interval: (16.0%, 21.4%), Median: 18.6%"
    },
    {
        "thought": "By letting different LLMs debate with each other, we can leverage their diverse perspectives to find better solutions for tasks.",
        "name": "LLM Debate",
        "code": "def forward(self, taskInfo):\n    # Instruction for initial reasoning\n    debate_initial_instruction = \"Please think step by step and then solve the task.\"\n\n    # Instruction for debating and updating the solution based on other agents' solutions\n    debate_instruction = \"Given solutions to the problem from other agents, consider their opinions as additional advice. Please think carefully and provide an updated answer.\"\n    \n    # Initialize debate agents with different roles and a moderate temperature for varied reasoning\n    debate_agents = [LLMAgentBase(['thinking', 'answer'], 'Debate Agent', temperature=0.8, role=role) for role in ['Math Professor', 'Grade School Teacher', 'Math Enthusiast']]\n\n    # Instruction for final decision-making based on all debates and solutions\n    final_decision_instruction = \"Given all the above thinking and answers, reason over them carefully and provide a final answer.\"\n    final_decision_agent = LLMAgentBase(['thinking', 'answer'], 'Final Decision Agent', temperature=0.1)\n\n    max_round = 2 # Maximum number of debate rounds\n    all_thinking = [[] for _ in range(max_round)]\n    all_answer = [[] for _ in range(max_round)]\n\n    # Perform debate rounds\n    for r in range(max_round):\n        for i in range(len(debate_agents)):\n            if r == 0:\n                thinking, answer = debate_agents[i]([taskInfo], debate_initial_instruction)\n            else:\n                input_infos = [taskInfo] + [all_thinking[r-1][i]] + all_thinking[r-1][:i] + all_thinking[r-1][i+1:]\n                thinking, answer = debate_agents[i](input_infos, debate_instruction)\n            all_thinking[r].append(thinking)\n            all_answer[r].append(answer)\n    \n    # Make the final decision based on all debate results and solutions\n    thinking, answer = final_decision_agent([taskInfo] + all_thinking[max_round-1] + all_answer[max_round-1], final_decision_instruction)\n    return answer\n",
        "api_calls": 12,
        "structure_label": "Multi-Agent Reasoning",
        "generation": "initial",
        "fitness": "95% Bootstrap Confidence Interval: (36.7%, 53.9%), Median: 45.3%",
        "test_fitness": "95% Bootstrap Confidence Interval: (44.8%, 51.7%), Median: 48.2%"
    },
    {
        "thought": "Let LLM first think about the principles involved in solving this task which could be helpful. By understanding the underlying principles, the model can better reason through the problem and provide a more accurate solution.",
        "name": "Step-back Abstraction",
        "code": "def forward(self, taskInfo):\n        # Instruction for understanding the principles involved in the task\n        principle_instruction = \"What are the physics, chemistry or biology principles and concepts involved in solving this task? First think step by step. Then list all involved principles and explain them.\"\n        \n        # Instruction for solving the task based on the principles\n        cot_instruction = \"Given the question and the involved principle behind the question, think step by step and then solve the task.\"\n        \n        # Instantiate LLM agents\n        principle_agent = LLMAgentBase(['thinking', 'principle'], 'Principle Agent')\n        cot_agent = LLMAgentBase(['thinking', 'answer'], 'Chain-of-Thought Agent')\n        \n        # Get the principles involved in the task\n        thinking, principle = principle_agent([taskInfo], principle_instruction)\n\n        # Use the principles to solve the task\n        thinking, answer = cot_agent([taskInfo, thinking, principle], cot_instruction)\n        return answer\n",
        "api_calls": 4,
        "structure_label": "Abstraction to Principles Reasoning",
        "generation": "initial",
        "fitness": "95% Bootstrap Confidence Interval: (14.8%, 28.9%), Median: 21.9%",
        "test_fitness": "95% Bootstrap Confidence Interval: (22.5%, 28.5%), Median: 25.5%"
    },
    {
        "thought": "Similar to Quality-Diversity methods, let LLM generate multiple diverse interesting solutions could help. By encouraging the model to explore different reasoning paths, we can increase the chances of finding the best solution.",
        "name": "Quality-Diversity",
        "code": "def forward(self, taskInfo):\n    # Instruction for initial reasoning\n    cot_initial_instruction = \"Please think step by step and then solve the task.\"\n\n    # Instruction for giving diverse answers\n    qd_instruction = \"Given previous attempts, try to come up with another interesting way to solve the task.\"\n    cot_agent = LLMAgentBase(['thinking', 'answer'], 'Chain-of-Thought Agent')\n\n    # Instruction for final decision-making based on collected reasoning and answers\n    final_decision_instruction = \"Given all the above solutions, reason over them carefully and provide a final answer.\"\n    final_decision_agent = LLMAgentBase(['thinking', 'answer'], 'Final Decision Agent', temperature=0.1)\n    \n    N_max = 3 # Maximum number of attempts\n\n    # Initial attempt\n    cot_inputs = [taskInfo]\n    possible_answers = []\n    thinking, answer = cot_agent(cot_inputs, cot_initial_instruction, 0)\n\n    # Add the answer to the list of possible answers\n    possible_answers.extend([thinking, answer])\n\n    for i in range(N_max):\n        # Reflect on previous attempts and generate another interesting answer\n        cot_inputs.extend([thinking, answer])\n\n        # Generate another interesting answer\n        thinking, answer = cot_agent(cot_inputs, qd_instruction, i + 1)\n        possible_answers.extend([thinking, answer])\n\n    # Make the final decision based on all generated answers\n    thinking, answer = final_decision_agent([taskInfo] + possible_answers, final_decision_instruction)\n    return answer\n",
        "api_calls": 8,
        "structure_label": "Multi-Agent Reasoning",
        "generation": "initial",
        "fitness": "95% Bootstrap Confidence Interval: (38.3%, 55.5%), Median: 46.9%",
        "test_fitness": "95% Bootstrap Confidence Interval: (51.1%, 58.0%), Median: 54.6%"
    },
    {
        "thought": "Similar to Auto-GPT and expert prompting, we can use dynamic control flow in the design to let the agent decide what expert we should use.",
        "name": "Dynamic Assignment of Roles",
        "code": "def forward(self, taskInfo):\n        # Instruction for step-by-step reasoning\n        cot_instruction = \"Please think step by step and then solve the task.\"\n        expert_agents = [LLMAgentBase(['thinking', 'answer'], 'Expert Agent', role=role) for role in ['Math Professor', 'Grade School Teacher', 'Math Enthusiast', 'Helpful Assistant']]\n\n        # Instruction for routing the task to the appropriate expert\n        routing_instruction = \"Given the task, please choose an Expert to answer the question. Choose from: Math Professor, Grade School Teacher, Math Enthusiast.\"\n        routing_agent = LLMAgentBase(['choice'], 'Routing agent')\n\n        # Get the choice of expert to route the task\n        choice = routing_agent([taskInfo], routing_instruction)[0]\n\n        if 'professor' in choice.content.lower():\n            expert_id = 0\n        elif 'teacher' in choice.content.lower():\n            expert_id = 1\n        elif 'enthusiast' in choice.content.lower():\n            expert_id = 2\n        else:\n            expert_id = 3 # Default to helpful assistant\n\n        thinking, answer = expert_agents[expert_id]([taskInfo], cot_instruction)\n        return answer\n",
        "api_calls": 6,
        "structure_label": "Decompositional Reasoning",
        "generation": "initial",
        "fitness": "95% Bootstrap Confidence Interval: (7.0%, 18.8%), Median: 12.5%",
        "test_fitness": "95% Bootstrap Confidence Interval: (10.8%, 15.4%), Median: 13.0%"
    },
    {
        "thought": "**Insights:**\nI propose a more structured approach that maintains iterative refinement but reduces redundancy in feedback processing. By directly refining the answer in each iteration instead of accumulating feedback, I can enhance the agent's performance while keeping the number of API calls efficient.\n\n**Overall Idea:**\nThis architecture will employ a single agent that iteratively refines its answer based on the output from the previous iteration. Each iteration will clearly state the task and provide a specific directive for refining the response, allowing for more focused and effective updates.\n\n**Implementation:**\n1. Generate the initial answer based on the task input.\n2. Use a loop to iteratively refine this answer, with each iteration providing direct feedback based on the previous output.\n3. Limit the number of iterations to maintain efficiency and ensure low API call counts.",
        "name": "Focused Iterative Refinement Agent",
        "code": "def forward(self, taskInfo):\n    # Step 1: Initial reasoning to produce the first solution\n    initial_instruction = \"Analyze the problem and provide an initial answer.\"\n    agent = LLMAgentBase([\"thinking\", \"final_answer\"], \"Focused Iterative Refinement Agent\")\n    \n    # Generate initial answer\n    output_info = agent([taskInfo], initial_instruction)  # 1 API call\n    refined_answer = output_info[1]\n    max_iterations = 3  # Limit the number of iterations for refinement\n\n    # Step 2: Iterative refinement based on previous output\n    for iterations in range(max_iterations):\n        # Create a new instance for each refinement iteration to maintain separate counts\n        refinement_agent = LLMAgentBase([\"thinking\", \"refined_answer\"], \"Refinement Agent\")\n        refinement_instruction = \"Refine your answer: {}. Ensure correctness and clarity in your response.\".format(refined_answer)\n        refined_output_info = refinement_agent([taskInfo, refined_answer], refinement_instruction)  # 1 API call each iteration\n        refined_answer = refined_output_info[1]  # Update with the newly refined answer\n    \n    # Return the final refined answer\n    return refined_answer  # Total API calls: 1 + 3 = 4",
        "fitness": "95% Bootstrap Confidence Interval: (78.9%, 90.6%), Median: 85.2%",
        "generation": 44,
        "api_calls": 4,
        "structure_label": "Iterative Refinement",
        "test_fitness": "95% Bootstrap Confidence Interval: (76.5%, 82.1%), Median: 79.4%"
    },
    {
        "thought": "**Insights:**\nTo improve upon the existing architecture, I propose a more integrated multi-agent approach that combines analysis and validation into a single cohesive process. This will reduce redundancy while still allowing for a thorough examination of the problem. \n\n**Overall Idea:**\nThe new architecture will employ a single agent for analysis and validation that responds iteratively. Instead of two separate agents, we will create an agent that refines its answer based on built-in validation checks throughout the process, allowing for more fluid corrections and improvements in a single workflow. This will reduce the number of API calls while maintaining depth in reasoning without compromising on performance.\n\n**Implementation:**\n1. Start with the initial analysis to produce a structured breakdown of the problem. \n2. Integrate validation checks within the same agent to assess clarity and correctness in real-time during the analysis phase. \n3. Use iterative refinement to enhance the answer based on validation feedback until reaching a satisfactory result or a maximum number of iterations.",
        "name": "Integrated Multi-Agent Analysis and Validation",
        "code": "def forward(self, taskInfo):\n    # Step 1: Initial analysis\n    initial_instruction = \"Analyze the math problem and provide a structured breakdown.\"\n    analysis_agent = LLMAgentBase([\"thinking\", \"analysis_output\"], \"Analysis Agent\")\n    analysis_output_info = analysis_agent([taskInfo], initial_instruction)  # 1 API call\n    refined_answer = analysis_output_info[1] \n    max_iterations = 5  # Allow for iterative refinement\n\n    # Step 2: Iterative refinement based on built-in validation checks\n    for iteration in range(max_iterations):\n        refinement_instruction = \"Refine your answer for clarity and correctness: {}\".format(refined_answer)\n        refinement_agent = LLMAgentBase([\"thinking\", \"refined_answer\"], \"Refinement Agent\")\n        refined_output_info = refinement_agent([taskInfo, refined_answer], refinement_instruction)  # 1 API call per iteration\n        refined_answer = refined_output_info[1]  # Update with newly refined answer\n    \n    return refined_answer  # Total API calls: 1 + 5 = 6",
        "fitness": "95% Bootstrap Confidence Interval: (76.6%, 89.8%), Median: 83.6%",
        "generation": 47,
        "api_calls": 6,
        "structure_label": "Iterative Refinement",
        "test_fitness": "95% Bootstrap Confidence Interval: (77.0%, 82.5%), Median: 79.8%"
    },
    {
        "thought": "**Insights:**\nTo improve the effectiveness of the architecture, I propose a structure that separates each function into distinct agents\u2014one for initial reasoning, another for iterative refinement, and a third for validation. This would enhance specialization and potentially lead to better outcomes. The iterative refinement can also be controlled to limit the number of calls.\n\n**Overall Idea:**\nThis architecture will include three distinct agents: the initial reasoning agent will provide an initial solution, the refinement agent will iteratively improve this solution, and the validation agent will assess the accuracy of the refined output. By ensuring that agents focus on their specific roles, we can enhance performance.\n\n**Implementation:**\n1. The initial reasoning agent generates the first solution.\n2. The refinement agent improves the solution iteratively based on the previous output.\n3. The validation agent checks the correctness of the refined output and provides feedback before returning the final answer, ensuring no re-use of the same agent across different tasks.",
        "name": "Multi-Agent Specialization Architecture",
        "code": "def forward(self, taskInfo):\n    # Step 1: Initial reasoning to produce the first solution\n    initial_instruction = \"Analyze the problem and propose a solution.\"\n    reasoning_agent = LLMAgentBase([\"thinking\", \"initial_answer\"], \"Reasoning Agent\")\n    initial_output = reasoning_agent([taskInfo], initial_instruction)  # 1 API call\n    \n    # Step 2: Refinement and validation of the initial answer\n    refinement_instruction = \"Refine the initial answer and validate the result.\"\n    refinement_agent = LLMAgentBase([\"thinking\", \"refined_answer\"], \"Refinement Agent\")\n    refined_output_info = refinement_agent([taskInfo, initial_output[1]], refinement_instruction)  # 2nd API call\n    \n    # Return the final refined answer\n    return refined_output_info[1]  # Total API calls: 2 (1 initial + 1 refinement/validation)",
        "fitness": "95% Bootstrap Confidence Interval: (68.8%, 83.6%), Median: 76.6%",
        "generation": 37,
        "api_calls": 2,
        "structure_label": "Multi-Agent Reasoning",
        "test_fitness": "95% Bootstrap Confidence Interval: (71.1%, 77.2%), Median: 74.2%"
    },
    {
        "thought": "**Insights:**\nTo make the architectural design more effective, I propose a structure that incorporates iterative refinement while maintaining a reduced number of API calls. This improvement seeks to enhance accuracy through multiple evaluations while still being efficient.\n\n**Overall Idea:**\nThe architecture will consist of one initial solution agent followed by two distinct refinement agents, making it capable of producing refined outputs based on previous results, while ensuring a low number of API calls.\n\n**Implementation:**\n1. Define an initial reasoning agent to solve the problem and provide the first output.\n2. Implement separate refinement agents that receive the initial output and refine it multiple times.\n3. Ensure that the total number of API calls remains within the specified limit by limiting the number of refinement iterations.",
        "name": "Iterative Refinement with Separate Agents",
        "code": "def forward(self, taskInfo):\n    # Initial instruction for the reasoning agent\n    initial_instruction = \"Analyze the problem and propose a solution.\"\n    initial_agent = LLMAgentBase(['thinking', 'initial_answer'], 'Initial Solution Agent')\n    \n    # Get the initial output from the initial agent\n    initial_output = initial_agent([taskInfo], initial_instruction)  # 1 API call\n    \n    # First refinement\n    refinement_instruction_1 = \"Refine the initial answer based on the following context.\"\n    refinement_agent_1 = LLMAgentBase(['thinking', 'refined_answer'], 'Refinement Agent 1')\n    refined_output_1 = refinement_agent_1([taskInfo, initial_output[1]], refinement_instruction_1)  # 2nd API call\n    \n    # Second refinement\n    refinement_instruction_2 = \"Further refine the previous answer.\"\n    refinement_agent_2 = LLMAgentBase(['thinking', 'refined_answer'], 'Refinement Agent 2')\n    refined_output_2 = refinement_agent_2([taskInfo, refined_output_1[1]], refinement_instruction_2)  # 3rd API call\n    \n    # Return the final refined answer\n    return refined_output_2[1]  # Total API calls: 1 (initial) + 1 (1st refinement) + 1 (2nd refinement) = 3 calls.",
        "fitness": "95% Bootstrap Confidence Interval: (64.1%, 79.7%), Median: 71.9%",
        "generation": 18,
        "api_calls": 3,
        "structure_label": "Iterative Refinement",
        "test_fitness": "95% Bootstrap Confidence Interval: (68.8%, 75.0%), Median: 71.9%"
    },
    {
        "thought": "**Insights:**\nTo enhance the proposed architecture further, I will introduce an additional layer where the initial agent not only proposes an answer but also outlines the reasoning steps leading to that answer. This will provide more context for the refinement agent and ensure it is working with a clearer understanding of the problem.\n\n**Overall Idea:**\nThe architecture will consist of the initial reasoning agent that breaks down the problem into reasoning steps before proposing a preliminary solution, followed by a refinement agent that builds on this structured reasoning to improve the answer, and a validation agent that checks the final output.\n\n**Implementation:**\n1. Define an initial reasoning agent that outlines the problem-solving steps and provides a preliminary answer.\n2. Implement a refinement agent that uses these structured steps to enhance the proposed answer.\n3. Introduce a validation agent to ensure the correctness of the final result, confirming adherence to mathematical principles.",
        "name": "Structured Reasoning for Enhanced Problem Solving",
        "code": "def forward(self, taskInfo):\n    # Step 1: Initial analysis of the problem and reasoning steps\n    initial_instruction = \"Analyze the problem and outline reasoning steps leading to a solution.\"\n    initial_agent = LLMAgentBase(['thinking', 'reasoning_steps', 'initial_answer'], 'Initial Reasoning Agent')\n    initial_output = initial_agent([taskInfo], initial_instruction)  # 1 API call\n\n    # Step 2: Detailed refinement of the proposed solution based on reasoning\n    # Extract the reasoning steps and initial answer\n    reasoning_steps = initial_output[1]  # Extract reasoning steps from Info output\n    initial_answer = initial_output[2]  # Extract initial answer from Info output\n\n    refinement_instruction = \"Refine the initial answer considering the following reasoning steps.\"\n    refinement_agent = LLMAgentBase(['thinking', 'refined_answer'], 'Refinement Agent')\n    refined_output = refinement_agent([taskInfo, reasoning_steps, initial_answer], refinement_instruction)  # 2nd API call\n\n    # Step 3: Validation of the refined answer\n    validation_instruction = \"Validate the following refined answer and confirm its correctness.\"\n    validation_agent = LLMAgentBase(['thinking', 'validated_answer'], 'Validation Agent')\n    validated_output = validation_agent([taskInfo, refined_output[1]], validation_instruction)  # 3rd API call\n\n    # Compiling the final response\n    return validated_output[1]  # Return validated answer; Total API calls: 1 (initial) + 1 (refinement) + 1 (validation) = 3 calls.",
        "fitness": "95% Bootstrap Confidence Interval: (57.8%, 74.2%), Median: 66.4%",
        "generation": 19,
        "api_calls": 3,
        "structure_label": "Iterative Refinement",
        "test_fitness": "95% Bootstrap Confidence Interval: (58.2%, 65.0%), Median: 61.6%"
    }
]