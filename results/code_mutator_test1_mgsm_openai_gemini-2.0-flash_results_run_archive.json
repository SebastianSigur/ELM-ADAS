[
    {
        "thought": "By encouraging the LLM to think step by step rather than directly outputting an answer, chain-of-thought reasoning enables complex problem-solving through intermediate steps. This practice improves the model's ability to handle tasks that require deeper reasoning and provides insight into its decision-making process.",
        "name": "Chain-of-Thought",
        "code": "def forward(self, taskInfo):\n    # Instruction for the Chain-of-Thought (CoT) approach\n    # It is an important practice that allows the LLM to think step by step before solving the task.\n    cot_instruction = \"Please think step by step and then solve the task.\"\n\n    # Instantiate a new LLM agent specifically for CoT\n    # To allow LLM thinking before answering, we need to set an additional output field 'thinking'.\n    cot_agent = LLMAgentBase(['thinking', 'answer'], 'Chain-of-Thought Agent')\n\n    # Prepare the inputs for the CoT agent\n    # The input should be a list of Info, and the first one is often the taskInfo\n    cot_agent_inputs = [taskInfo]\n\n    # Get the response from the CoT agent\n    thinking, answer = cot_agent(cot_agent_inputs, cot_instruction)\n\n    # Return only the final answer\n    return answer\n",
        "generation": "initial",
        "fitness": "95% Bootstrap Confidence Interval: (71.1%, 85.2%), Median: 78.1%"
    },
    {
        "thought": "While an LLM can arrive at the correct answer, its reasoning may vary. By repeatedly asking the same question with high temperature settings, we can generate different reasoning paths. We then combine multiple answers from these Chain-of-Thought (CoT) agents to produce a more accurate final answer through ensembling.",
        "name": "Self-Consistency with Chain-of-Thought",
        "code": "def forward(self, taskInfo):\n    # Instruction for step-by-step reasoning\n    cot_instruction = \"Please think step by step and then solve the task.\"\n    N = 5 # Number of CoT agents\n\n    # Initialize multiple CoT agents with a higher temperature for varied reasoning\n    cot_agents = [LLMAgentBase(['thinking', 'answer'], 'Chain-of-Thought Agent', temperature=0.8) for _ in range(N)]\n\n    # Majority voting function to select the most common answer\n    from collections import Counter\n    def majority_voting(answers):\n        return Counter(answers).most_common(1)[0][0]\n    \n    possible_answers = []\n    for i in range(N):\n        thinking, answer = cot_agents[i]([taskInfo], cot_instruction)\n        possible_answers.append(answer.content)\n\n    # Ensembling the answers from multiple CoT agents\n    answer = majority_voting(possible_answers)\n    return answer  \n",
        "generation": "initial",
        "fitness": "95% Bootstrap Confidence Interval: (74.2%, 87.5%), Median: 81.2%"
    },
    {
        "thought": "To enhance its performance, an LLM can iteratively improve its answer based on feedback. By reflecting on its previous attempts and incorporating feedback, the model can refine its reasoning and provide a more accurate solution.",
        "name": "Self-Refine (Reflexion)",
        "code": "def forward(self, taskInfo):\n    # Instruction for initial reasoning\n    cot_initial_instruction = \"Please think step by step and then solve the task.\"\n\n    # Instruction for reflecting on previous attempts and feedback to improve\n    cot_reflect_instruction = \"Given previous attempts and feedback, carefully consider where you could go wrong in your latest attempt. Using insights from previous attempts, try to solve the task better.\"\n    cot_agent = LLMAgentBase(['thinking', 'answer'], 'Chain-of-Thought Agent')\n\n    # Instruction for providing feedback and correcting the answer\n    critic_instruction = \"Please review the answer above and criticize on where might be wrong. If you are absolutely sure it is correct, output 'True' in 'correct'.\"\n    critic_agent = LLMAgentBase(['feedback', 'correct'], 'Critic Agent')\n    \n    N_max = 5 # Maximum number of attempts\n\n    # Initial attempt\n    cot_inputs = [taskInfo]\n    thinking, answer = cot_agent(cot_inputs, cot_initial_instruction, 0)\n\n    for i in range(N_max):\n        # Get feedback and correct status from the critic\n        feedback, correct = critic_agent([taskInfo, thinking, answer], critic_instruction, i)\n        if correct.content == 'True':\n            break\n            \n        # Add feedback to the inputs for the next iteration\n        cot_inputs.extend([thinking, answer, feedback])\n\n        # Reflect on previous attempts and refine the answer\n        thinking, answer = cot_agent(cot_inputs, cot_reflect_instruction, i + 1)\n    return answer\n",
        "generation": "initial",
        "fitness": "95% Bootstrap Confidence Interval: (82.8%, 93.8%), Median: 88.3%"
    },
    {
        "thought": "By letting different LLMs debate with each other, we can leverage their diverse perspectives to find better solutions for tasks.",
        "name": "LLM Debate",
        "code": "def forward(self, taskInfo):\n    # Instruction for initial reasoning\n    debate_initial_instruction = \"Please think step by step and then solve the task.\"\n\n    # Instruction for debating and updating the solution based on other agents' solutions\n    debate_instruction = \"Given solutions to the problem from other agents, consider their opinions as additional advice. Please think carefully and provide an updated answer.\"\n    \n    # Initialize debate agents with different roles and a moderate temperature for varied reasoning\n    debate_agents = [LLMAgentBase(['thinking', 'answer'], 'Debate Agent', temperature=0.8, role=role) for role in ['Math Professor', 'Grade School Teacher', 'Math Enthusiast']]\n\n    # Instruction for final decision-making based on all debates and solutions\n    final_decision_instruction = \"Given all the above thinking and answers, reason over them carefully and provide a final answer.\"\n    final_decision_agent = LLMAgentBase(['thinking', 'answer'], 'Final Decision Agent', temperature=0.1)\n\n    max_round = 2 # Maximum number of debate rounds\n    all_thinking = [[] for _ in range(max_round)]\n    all_answer = [[] for _ in range(max_round)]\n\n    # Perform debate rounds\n    for r in range(max_round):\n        for i in range(len(debate_agents)):\n            if r == 0:\n                thinking, answer = debate_agents[i]([taskInfo], debate_initial_instruction)\n            else:\n                input_infos = [taskInfo] + [all_thinking[r-1][i]] + all_thinking[r-1][:i] + all_thinking[r-1][i+1:]\n                thinking, answer = debate_agents[i](input_infos, debate_instruction)\n            all_thinking[r].append(thinking)\n            all_answer[r].append(answer)\n    \n    # Make the final decision based on all debate results and solutions\n    thinking, answer = final_decision_agent([taskInfo] + all_thinking[max_round-1] + all_answer[max_round-1], final_decision_instruction)\n    return answer\n",
        "generation": "initial",
        "fitness": "95% Bootstrap Confidence Interval: (85.9%, 96.1%), Median: 91.4%"
    },
    {
        "thought": "Let LLM first think about the principles involved in solving this task which could be helpful. By understanding the underlying principles, the model can better reason through the problem and provide a more accurate solution.",
        "name": "Step-back Abstraction",
        "code": "def forward(self, taskInfo):\n        # Instruction for understanding the principles involved in the task\n        principle_instruction = \"What are the physics, chemistry or biology principles and concepts involved in solving this task? First think step by step. Then list all involved principles and explain them.\"\n        \n        # Instruction for solving the task based on the principles\n        cot_instruction = \"Given the question and the involved principle behind the question, think step by step and then solve the task.\"\n        \n        # Instantiate LLM agents\n        principle_agent = LLMAgentBase(['thinking', 'principle'], 'Principle Agent')\n        cot_agent = LLMAgentBase(['thinking', 'answer'], 'Chain-of-Thought Agent')\n        \n        # Get the principles involved in the task\n        thinking, principle = principle_agent([taskInfo], principle_instruction)\n\n        # Use the principles to solve the task\n        thinking, answer = cot_agent([taskInfo, thinking, principle], cot_instruction)\n        return answer\n",
        "generation": "initial",
        "fitness": "95% Bootstrap Confidence Interval: (78.9%, 91.4%), Median: 85.2%"
    },
    {
        "thought": "Similar to Quality-Diversity methods, let LLM generate multiple diverse interesting solutions could help. By encouraging the model to explore different reasoning paths, we can increase the chances of finding the best solution.",
        "name": "Quality-Diversity",
        "code": "def forward(self, taskInfo):\n    # Instruction for initial reasoning\n    cot_initial_instruction = \"Please think step by step and then solve the task.\"\n\n    # Instruction for giving diverse answers\n    qd_instruction = \"Given previous attempts, try to come up with another interesting way to solve the task.\"\n    cot_agent = LLMAgentBase(['thinking', 'answer'], 'Chain-of-Thought Agent')\n\n    # Instruction for final decision-making based on collected reasoning and answers\n    final_decision_instruction = \"Given all the above solutions, reason over them carefully and provide a final answer.\"\n    final_decision_agent = LLMAgentBase(['thinking', 'answer'], 'Final Decision Agent', temperature=0.1)\n    \n    N_max = 3 # Maximum number of attempts\n\n    # Initial attempt\n    cot_inputs = [taskInfo]\n    possible_answers = []\n    thinking, answer = cot_agent(cot_inputs, cot_initial_instruction, 0)\n\n    # Add the answer to the list of possible answers\n    possible_answers.extend([thinking, answer])\n\n    for i in range(N_max):\n        # Reflect on previous attempts and generate another interesting answer\n        cot_inputs.extend([thinking, answer])\n\n        # Generate another interesting answer\n        thinking, answer = cot_agent(cot_inputs, qd_instruction, i + 1)\n        possible_answers.extend([thinking, answer])\n\n    # Make the final decision based on all generated answers\n    thinking, answer = final_decision_agent([taskInfo] + possible_answers, final_decision_instruction)\n    return answer\n",
        "generation": "initial",
        "fitness": "95% Bootstrap Confidence Interval: (83.6%, 94.5%), Median: 89.1%"
    },
    {
        "thought": "Similar to Auto-GPT and expert prompting, we can use dynamic control flow in the design to let the agent decide what expert we should use.",
        "name": "Dynamic Assignment of Roles",
        "code": "def forward(self, taskInfo):\n        # Instruction for step-by-step reasoning\n        cot_instruction = \"Please think step by step and then solve the task.\"\n        expert_agents = [LLMAgentBase(['thinking', 'answer'], 'Expert Agent', role=role) for role in ['Math Professor', 'Grade School Teacher', 'Math Enthusiast', 'Helpful Assistant']]\n\n        # Instruction for routing the task to the appropriate expert\n        routing_instruction = \"Given the task, please choose an Expert to answer the question. Choose from: Math Professor, Grade School Teacher, Math Enthusiast.\"\n        routing_agent = LLMAgentBase(['choice'], 'Routing agent')\n\n        # Get the choice of expert to route the task\n        choice = routing_agent([taskInfo], routing_instruction)[0]\n\n        if 'professor' in choice.content.lower():\n            expert_id = 0\n        elif 'teacher' in choice.content.lower():\n            expert_id = 1\n        elif 'enthusiast' in choice.content.lower():\n            expert_id = 2\n        else:\n            expert_id = 3 # Default to helpful assistant\n\n        thinking, answer = expert_agents[expert_id]([taskInfo], cot_instruction)\n        return answer\n",
        "generation": "initial",
        "fitness": "95% Bootstrap Confidence Interval: (26.6%, 43.0%), Median: 34.4%"
    },
    {
        "thought": "**Insights:**\nTo further enhance the effectiveness of the collaborative reasoning approach, I propose implementing a structured consensus mechanism where each agent's input is not only discussed but also scored based on predefined criteria. This scoring will allow the final decision agent to prioritize insights from the most relevant agents, thereby improving the overall quality of the final answer.\n\n**Overall Idea:**\nThe revised architecture will still involve specialized agents collaborating to solve a mathematical problem, but now each agent will provide a score for their reasoning based on predefined criteria, which will then be used by the final decision agent to reach a consensus.\n\n**Implementation:**\n1. **Define Scoring Criteria:** Establish a method for agents to rate their peers\u2019 contributions.\n2. **Adjust Dialogue and Scoring:** Modify the collaborative discussion phase to include scoring, ensuring each agent scores contributions based on relevance and clarity.\n3. **Final Decision Process:** The final decision agent will utilize these scores to weigh the different answers and arrive at a more informed consensus.",
        "name": "Collaborative Reasoning with Critique",
        "code": "def forward(self, taskInfo):\n    # Instructions for each specialized agent\n    algebra_instruction = \"Please reason about the algebraic aspects of the problem step by step and provide a score for your reasoning.\"\n    geometry_instruction = \"Please consider any geometric interpretations, reason step by step, and assign a score to your reasoning.\"\n    logic_instruction = \"Please apply logical reasoning to analyze the problem step by step and provide a scoring.\"\n    \n    # Create specialized agents\n    algebra_agent = LLMAgentBase(['thinking', 'answer', 'score'], 'Algebra Agent')\n    geometry_agent = LLMAgentBase(['thinking', 'answer', 'score'], 'Geometry Agent')\n    logic_agent = LLMAgentBase(['thinking', 'answer', 'score'], 'Logic Agent')\n    \n    # Each agent independently reasons about the task and scores their contributions\n    algebra_thinking, algebra_answer, algebra_score = algebra_agent([taskInfo], algebra_instruction)\n    geometry_thinking, geometry_answer, geometry_score = geometry_agent([taskInfo], geometry_instruction)\n    logic_thinking, logic_answer, logic_score = logic_agent([taskInfo], logic_instruction)\n    \n    # Compile the responses for collaborative discussion, using Info objects directly\n    discussion_inputs = [\n        taskInfo,\n        algebra_thinking,\n        algebra_answer,\n        algebra_score,\n        geometry_thinking,\n        geometry_answer,\n        geometry_score,\n        logic_thinking,\n        logic_answer,\n        logic_score\n    ]\n    discussion_instruction = \"Based on the reasoning and scores provided by each agent, discuss the answers and come to a consensus.\"\n    final_decision_agent = LLMAgentBase(['thinking', 'answer'], 'Final Decision Agent')\n    final_thinking, final_answer = final_decision_agent(discussion_inputs, discussion_instruction)\n    \n    return final_answer",
        "fitness": "95% Bootstrap Confidence Interval: (87.5%, 96.1%), Median: 92.2%",
        "generation": 1,
        "code_mutator": "# INSTRUCTION: Come up with another creative way to solve the problem."
    },
    {
        "thought": "**Insights:**\nTo enhance the existing architecture, a more dynamic engagement between specialized agents is necessary. Introducing a multi-round refinement process where agents critique and evolve each other\u2019s answers could create a more robust solution. Agents can propose alternative solutions to the same problem, which allows for a richer exploration of diverse reasoning paths.\n\n**Overall Idea:**\nThis architecture will consist of specialized agents that first independently reason and generate answers. They will then engage in a debate where they present their answers, critique each other, and refine their responses based on feedback. The final answer will emerge from this iterative process, emphasizing collaboration and diverse perspectives.\n\n**Implementation:**\n1. Each specialized agent will generate an initial response to the task.\n2. Agents will be tasked with critiquing the answers of their peers, discussing strengths and weaknesses, and proposing improvements.\n3. This debate will continue for a set number of rounds, ensuring a richer understanding and better solutions.\n4. A final decision agent will synthesize the critiques and improved answers to arrive at the best solution.",
        "name": "Collaborative Reasoning with Adaptive Feedback",
        "code": "def forward(self, taskInfo):\n    # Step 1: Initial reasoning instructions for specialized agents\n    reasoning_instruction = \"Please reason about the problem step by step and provide your answer.\"\n    critique_instruction = \"Please read the answers provided by your peers, critique them, and suggest improvements.\"\n\n    # Step 2: Create specialized agents\n    algebra_agent = LLMAgentBase(['thinking', 'answer'], 'Algebra Agent')\n    geometry_agent = LLMAgentBase(['thinking', 'answer'], 'Geometry Agent')\n    logic_agent = LLMAgentBase(['thinking', 'answer'], 'Logic Agent')\n\n    # Step 3: Each agent generates initial answers\n    algebra_response = algebra_agent([taskInfo], reasoning_instruction)[0]\n    geometry_response = geometry_agent([taskInfo], reasoning_instruction)[0]\n    logic_response = logic_agent([taskInfo], reasoning_instruction)[0]\n\n    # Step 4: Debate and refinement rounds\n    max_rounds = 3\n    for i in range(max_rounds):\n        # Compile responses for critique\n        responses = [algebra_response, geometry_response, logic_response]\n        critiques = [\n            algebra_agent([taskInfo, responses], critique_instruction)[0],\n            geometry_agent([taskInfo, responses], critique_instruction)[0],\n            logic_agent([taskInfo, responses], critique_instruction)[0]\n        ]\n        # Aggregate critiques and update answers based on feedback\n        algebra_response = f\"{algebra_response}, {critiques[0].content}\" if hasattr(critiques[0], 'content') else f\"{algebra_response}, {critiques[0]}\"\n        geometry_response = f\"{geometry_response}, {critiques[1].content}\" if hasattr(critiques[1], 'content') else f\"{geometry_response}, {critiques[1]}\"\n        logic_response = f\"{logic_response}, {critiques[2].content}\" if hasattr(critiques[2], 'content') else f\"{logic_response}, {critiques[2]}\"\n    \n    # Step 5: Final decision based on refined answers\n    final_answers = [algebra_response, geometry_response, logic_response]\n    final_decision_agent = LLMAgentBase(['thinking', 'answer'], 'Final Decision Agent')\n    final_thinking, final_answer = final_decision_agent([taskInfo] + final_answers, \"Provide the best answer based on the refined responses.\")\n\n    return final_answer",
        "fitness": "95% Bootstrap Confidence Interval: (77.3%, 89.8%), Median: 83.6%",
        "generation": 2,
        "code_mutator": "# INSTRUCTION: Revamp the current Python code to create a complex structured solution that has never been seen before."
    },
    {
        "thought": "**Insights:**\nThe existing architecture can be enhanced by focusing on not just the critique of answers but also on incorporating a role-adjustment mechanism that allows agents to switch strategies based on performance evaluations of their answers and their peers'. This approach can further deepen the collaborative engagement and make the agents more adaptable to the challenges posed by the task.\n\n**Overall Idea:**\nThis architecture will consist of agents that first reason independently, then provide structured critiques that will not only evaluate their responses but also suggest strategic adjustments for next rounds. This dynamic evaluation will allow agents to reframe their approaches based on peer feedback, leading to more nuanced and accurate answers.\n\n**Implementation:**\n1. **Initial Reasoning:** Each agent will independently generate an answer.\n2. **Structured Critique:** Each agent will provide a detailed critique that includes suggestions for improving their answers and adjusting their strategies.\n3. **Role Adjustment:** Agents will use feedback to modify their reasoning approach in the next round.\n4. **Refinement:** Agents will iterate on their answers based on the adjusted strategies and critiques, leading to a collaborative refinement.",
        "name": "Scoring and Adaptive Collaboration Agent",
        "code": "def forward(self, taskInfo):\n    # Step 1: Initial reasoning instructions for specialized agents\n    reasoning_instruction = \"Please reason about the problem step by step and provide your answer.\"\n    critique_instruction = \"Critique your own answer and the answers of your peers, providing suggestions for improvement.\"\n\n    # Step 2: Create specialized agents\n    agents = [LLMAgentBase(['thinking', 'answer', 'critique'], f'Agent {i+1}') for i in range(3)]\n\n    # Step 3: Each agent generates initial answers\n    initial_answers = [agent([taskInfo], reasoning_instruction)[1] for agent in agents]\n\n    # Step 4: Structured critique phase\n    critiques = [agent([taskInfo] + initial_answers, critique_instruction) for agent in agents]\n    \n    # Step 5: Update answers based on critiques\n    updated_answers = []\n    for i, (answer, critique) in enumerate(zip(initial_answers, critiques)):\n        if critique[1]:  # Check if critique is valid (not None)\n            updated_answers.append(Info('answer', f'{answer.content} (Critique: {critique[1].content})', 'Agent {i+1}', 0))  # Enhanced feedback using Info\n        else:\n            updated_answers.append(answer)  # No feedback, retain original\n\n    # Step 6: Final decision based on refined answers\n    final_decision_agent = LLMAgentBase(['thinking', 'answer'], 'Final Decision Agent')\n    final_thinking, final_answer = final_decision_agent([taskInfo] + updated_answers, \"Determine the best answer based on the updates.\")\n\n    return final_answer",
        "fitness": "95% Bootstrap Confidence Interval: (67.2%, 82.0%), Median: 75.0%",
        "generation": 3,
        "code_mutator": "# INSTRUCTION: Act as an experienced Python programmer and LLM expert. Create a new solution that vastly improves the current one."
    },
    {
        "thought": "**Insights:**\nTo significantly enhance the collaborative reasoning process, a more integrated approach should be taken, combining brainstorming with an effective refinement strategy. In this new architecture, agents will first generate diverse solutions without critique. Following this, a synthesis process will merge these ideas and distill them into a coherent answer. This architecture emphasizes the importance of idea generation as a distinct and valuable part of problem-solving, rather than just focusing on critique and refinement.\n\n**Overall Idea:** \nThe architecture will consist of brainstorming agents that generate a wide array of ideas, followed by synthesis agents that review these ideas and combine the best elements into a final solution.\n\n**Implementation:**\n1. **Brainstorming Phase:** Define multiple agents that generate ideas based on the task. They will operate without restriction to promote creativity.\n2. **Synthesis Phase:** A synthesis agent will take the ideas generated, merging them into cohesive solutions while ensuring clarity and coherence in the final output.\n3. **Final Decisions:** A final decision agent will synthesize the work of the synthesis phase, ensuring the best ideas are included in the final answer.",
        "fitness": "95% Bootstrap Confidence Interval: (0.0%, 0.0%), Median: 0.0%",
        "generation": 4,
        "code_mutator": "# INSTRUCTION: Just change this code to make it more fun, think WELL outside the box."
    },
    {
        "thought": "**Insights:**\nTo create a more effective architecture, I suggest a 'Brainstorming and Voting Synthesis' agent where multiple specialized agents generate ideas independently and then participate in a voting process. The best ideas, determined through collective voting, will be synthesized into a cohesive final answer. This structure enhances collaboration by fostering a competitive yet constructive environment for idea generation, which can lead to higher quality outputs and more diverse reasoning paths.\n\n**Overall Idea:**\nThis architecture will consist of brainstorming agents that generate ideas, followed by a voting phase where each agent can vote on the best ideas. A synthesis agent will then merge these top ideas into a final solution, ensuring clarity and coherence.\n\n**Implementation:**\n1. **Brainstorming Phase:** Define multiple agents that generate ideas based on the task with no restrictions to promote creativity.\n2. **Voting Phase:** Implement a voting mechanism where agents can score the ideas generated by their peers, selecting the most promising ones.\n3. **Synthesis Phase:** A synthesis agent will take the top ideas and merge them into a cohesive final answer.\n4. **Final Decisions:** A final decision agent will ensure the best ideas are included in the final output.",
        "name": "Dynamic Idea Fusion",
        "code": "def forward(self, taskInfo):\n    # Step 1: Instructions for specialized agents\n    brainstorming_instruction = \"Please generate diverse and relevant ideas for the math problem step by step, focusing on clarity and correctness.\"\n    voting_instruction = \"Review the generated ideas and assign a numeric score to each based on relevance and clarity.\"\n    synthesis_instruction = \"Merge the selected top ideas into a cohesive final answer.\"\n    \n    # Step 2: Create specialized brainstorming agents\n    algebra_agent = LLMAgentBase([\"thinking\", \"answer\"], \"Algebra Brainstorming Agent\")\n    geometry_agent = LLMAgentBase([\"thinking\", \"answer\"], \"Geometry Brainstorming Agent\")\n    logic_agent = LLMAgentBase([\"thinking\", \"answer\"], \"Logic Brainstorming Agent\")\n    \n    # Step 3: Each agent generates initial ideas\n    algebra_ideas = algebra_agent([taskInfo], brainstorming_instruction)\n    geometry_ideas = geometry_agent([taskInfo], brainstorming_instruction)\n    logic_ideas = logic_agent([taskInfo], brainstorming_instruction)\n    \n    # Step 4: Collect all ideas\n    all_ideas = algebra_ideas + geometry_ideas + logic_ideas  # Flatten the list of ideas\n    \n    # Step 5: Voting Phase - Each agent votes on ideas\n    scores = {idea: [] for idea in all_ideas}\n    for idea_info in all_ideas:\n        # Ensure we are working with Info objects\n        for agent in [algebra_agent, geometry_agent, logic_agent]:\n            vote_responses = agent([taskInfo, idea_info], voting_instruction)  # Expecting a list of votes\n            # Verify that vote_responses is a list and has valid numeric scores\n            print(f\"Vote responses for {idea_info}: {vote_responses}\")  # Debugging statement\n            for vote_info in vote_responses:\n                if hasattr(vote_info, 'content') and vote_info.content.isnumeric():  # Validate that content is a number\n                    scores[idea_info].append(int(vote_info.content))  # Append score\n    \n    # Step 6: Calculate average scores for each idea\n    average_scores = {idea: sum(votes) / len(votes) if votes else 0 for idea, votes in scores.items()}\n    \n    # Step 7: Select the top ideas based on average scores\n    top_ideas = sorted(average_scores.items(), key=lambda x: x[1], reverse=True)[:3]  # Get top 3 ideas based on scores\n    \n    # Step 8: Synthesis Phase - Create cohesive final answer\n    synthesis_agent = LLMAgentBase([\"thinking\", \"answer\"], \"Final Synthesis Agent\")\n    final_answer_info = synthesis_agent([taskInfo] + [idea for idea, _ in top_ideas], synthesis_instruction)\n    \n    return final_answer_info",
        "fitness": "95% Bootstrap Confidence Interval: (0.0%, 0.0%), Median: 0.0%",
        "generation": 7,
        "code_mutator": "# INSTRUCTION: Change the code to solve the problem in a different way."
    }
]