[
    {
        "thought": "By encouraging the LLM to think step by step rather than directly outputting an answer, chain-of-thought reasoning enables complex problem-solving through intermediate steps. This practice improves the model's ability to handle tasks that require deeper reasoning and provides insight into its decision-making process.",
        "name": "Chain-of-Thought",
        "code": "def forward(self, taskInfo):\n    # Instruction for the Chain-of-Thought (CoT) approach\n    # It is an important practice that allows the LLM to think step by step before solving the task.\n    cot_instruction = \"Please think step by step and then solve the task.\"\n\n    # Instantiate a new LLM agent specifically for CoT\n    # To allow LLM thinking before answering, we need to set an additional output field 'thinking'.\n    cot_agent = LLMAgentBase(['thinking', 'answer'], 'Chain-of-Thought Agent')\n\n    # Prepare the inputs for the CoT agent\n    # The input should be a list of Info, and the first one is often the taskInfo\n    cot_agent_inputs = [taskInfo]\n\n    # Get the response from the CoT agent\n    thinking, answer = cot_agent(cot_agent_inputs, cot_instruction)\n\n    # Return only the final answer\n    return answer\n",
        "api_calls": 1,
        "structure_label": "Linear Chain-of-Thought",
        "generation": "initial",
        "fitness": "95% Bootstrap Confidence Interval: (11.7%, 25.0%), Median: 18.0%",
        "test_fitness": "95% Bootstrap Confidence Interval: (11.8%, 16.6%), Median: 14.1%"
    },
    {
        "thought": "While an LLM can arrive at the correct answer, its reasoning may vary. By repeatedly asking the same question with high temperature settings, we can generate different reasoning paths. We then combine multiple answers from these Chain-of-Thought (CoT) agents to produce a more accurate final answer through ensembling.",
        "name": "Self-Consistency with Chain-of-Thought",
        "code": "def forward(self, taskInfo):\n    # Instruction for step-by-step reasoning\n    cot_instruction = \"Please think step by step and then solve the task.\"\n    N = 5 # Number of CoT agents\n\n    # Initialize multiple CoT agents with a higher temperature for varied reasoning\n    cot_agents = [LLMAgentBase(['thinking', 'answer'], 'Chain-of-Thought Agent', temperature=0.8) for _ in range(N)]\n\n    # Majority voting function to select the most common answer\n    from collections import Counter\n    def majority_voting(answers):\n        return Counter(answers).most_common(1)[0][0]\n    \n    possible_answers = []\n    for i in range(N):\n        thinking, answer = cot_agents[i]([taskInfo], cot_instruction)\n        possible_answers.append(answer.content)\n\n    # Ensembling the answers from multiple CoT agents\n    answer = majority_voting(possible_answers)\n    return answer  \n",
        "api_calls": 5,
        "structure_label": "Multi-Agent Reasoning",
        "generation": "initial",
        "fitness": "95% Bootstrap Confidence Interval: (5.5%, 16.4%), Median: 10.9%",
        "test_fitness": "95% Bootstrap Confidence Interval: (11.9%, 16.8%), Median: 14.2%"
    },
    {
        "thought": "To enhance its performance, an LLM can iteratively improve its answer based on feedback. By reflecting on its previous attempts and incorporating feedback, the model can refine its reasoning and provide a more accurate solution.",
        "name": "Self-Refine (Reflexion)",
        "code": "def forward(self, taskInfo):\n    # Instruction for initial reasoning\n    cot_initial_instruction = \"Please think step by step and then solve the task.\"\n\n    # Instruction for reflecting on previous attempts and feedback to improve\n    cot_reflect_instruction = \"Given previous attempts and feedback, carefully consider where you could go wrong in your latest attempt. Using insights from previous attempts, try to solve the task better.\"\n    cot_agent = LLMAgentBase(['thinking', 'answer'], 'Chain-of-Thought Agent')\n\n    # Instruction for providing feedback and correcting the answer\n    critic_instruction = \"Please review the answer above and criticize on where might be wrong. If you are absolutely sure it is correct, output 'True' in 'correct'.\"\n    critic_agent = LLMAgentBase(['feedback', 'correct'], 'Critic Agent')\n    \n    N_max = 5 # Maximum number of attempts\n\n    # Initial attempt\n    cot_inputs = [taskInfo]\n    thinking, answer = cot_agent(cot_inputs, cot_initial_instruction, 0)\n\n    for i in range(N_max):\n        # Get feedback and correct status from the critic\n        feedback, correct = critic_agent([taskInfo, thinking, answer], critic_instruction, i)\n        if correct.content == 'True':\n            break\n            \n        # Add feedback to the inputs for the next iteration\n        cot_inputs.extend([thinking, answer, feedback])\n\n        # Reflect on previous attempts and refine the answer\n        thinking, answer = cot_agent(cot_inputs, cot_reflect_instruction, i + 1)\n    return answer\n",
        "api_calls": 10,
        "structure_label": "Iterative Refinement",
        "generation": "initial",
        "fitness": "95% Bootstrap Confidence Interval: (11.7%, 25.0%), Median: 18.0%",
        "test_fitness": "95% Bootstrap Confidence Interval: (15.1%, 20.5%), Median: 17.8%"
    },
    {
        "thought": "By letting different LLMs debate with each other, we can leverage their diverse perspectives to find better solutions for tasks.",
        "name": "LLM Debate",
        "code": "def forward(self, taskInfo):\n    # Instruction for initial reasoning\n    debate_initial_instruction = \"Please think step by step and then solve the task.\"\n\n    # Instruction for debating and updating the solution based on other agents' solutions\n    debate_instruction = \"Given solutions to the problem from other agents, consider their opinions as additional advice. Please think carefully and provide an updated answer.\"\n    \n    # Initialize debate agents with different roles and a moderate temperature for varied reasoning\n    debate_agents = [LLMAgentBase(['thinking', 'answer'], 'Debate Agent', temperature=0.8, role=role) for role in ['Math Professor', 'Grade School Teacher', 'Math Enthusiast']]\n\n    # Instruction for final decision-making based on all debates and solutions\n    final_decision_instruction = \"Given all the above thinking and answers, reason over them carefully and provide a final answer.\"\n    final_decision_agent = LLMAgentBase(['thinking', 'answer'], 'Final Decision Agent', temperature=0.1)\n\n    max_round = 2 # Maximum number of debate rounds\n    all_thinking = [[] for _ in range(max_round)]\n    all_answer = [[] for _ in range(max_round)]\n\n    # Perform debate rounds\n    for r in range(max_round):\n        for i in range(len(debate_agents)):\n            if r == 0:\n                thinking, answer = debate_agents[i]([taskInfo], debate_initial_instruction)\n            else:\n                input_infos = [taskInfo] + [all_thinking[r-1][i]] + all_thinking[r-1][:i] + all_thinking[r-1][i+1:]\n                thinking, answer = debate_agents[i](input_infos, debate_instruction)\n            all_thinking[r].append(thinking)\n            all_answer[r].append(answer)\n    \n    # Make the final decision based on all debate results and solutions\n    thinking, answer = final_decision_agent([taskInfo] + all_thinking[max_round-1] + all_answer[max_round-1], final_decision_instruction)\n    return answer\n",
        "api_calls": 12,
        "structure_label": "Multi-Agent Reasoning",
        "generation": "initial",
        "fitness": "95% Bootstrap Confidence Interval: (38.3%, 55.5%), Median: 46.9%",
        "test_fitness": "95% Bootstrap Confidence Interval: (44.4%, 51.4%), Median: 47.9%"
    },
    {
        "thought": "Let LLM first think about the principles involved in solving this task which could be helpful. By understanding the underlying principles, the model can better reason through the problem and provide a more accurate solution.",
        "name": "Step-back Abstraction",
        "code": "def forward(self, taskInfo):\n        # Instruction for understanding the principles involved in the task\n        principle_instruction = \"What are the physics, chemistry or biology principles and concepts involved in solving this task? First think step by step. Then list all involved principles and explain them.\"\n        \n        # Instruction for solving the task based on the principles\n        cot_instruction = \"Given the question and the involved principle behind the question, think step by step and then solve the task.\"\n        \n        # Instantiate LLM agents\n        principle_agent = LLMAgentBase(['thinking', 'principle'], 'Principle Agent')\n        cot_agent = LLMAgentBase(['thinking', 'answer'], 'Chain-of-Thought Agent')\n        \n        # Get the principles involved in the task\n        thinking, principle = principle_agent([taskInfo], principle_instruction)\n\n        # Use the principles to solve the task\n        thinking, answer = cot_agent([taskInfo, thinking, principle], cot_instruction)\n        return answer\n",
        "api_calls": 4,
        "structure_label": "Abstraction to Principles Reasoning",
        "generation": "initial",
        "fitness": "95% Bootstrap Confidence Interval: (12.5%, 25.8%), Median: 18.8%",
        "test_fitness": "95% Bootstrap Confidence Interval: (21.8%, 27.8%), Median: 24.8%"
    },
    {
        "thought": "Similar to Quality-Diversity methods, let LLM generate multiple diverse interesting solutions could help. By encouraging the model to explore different reasoning paths, we can increase the chances of finding the best solution.",
        "name": "Quality-Diversity",
        "code": "def forward(self, taskInfo):\n    # Instruction for initial reasoning\n    cot_initial_instruction = \"Please think step by step and then solve the task.\"\n\n    # Instruction for giving diverse answers\n    qd_instruction = \"Given previous attempts, try to come up with another interesting way to solve the task.\"\n    cot_agent = LLMAgentBase(['thinking', 'answer'], 'Chain-of-Thought Agent')\n\n    # Instruction for final decision-making based on collected reasoning and answers\n    final_decision_instruction = \"Given all the above solutions, reason over them carefully and provide a final answer.\"\n    final_decision_agent = LLMAgentBase(['thinking', 'answer'], 'Final Decision Agent', temperature=0.1)\n    \n    N_max = 3 # Maximum number of attempts\n\n    # Initial attempt\n    cot_inputs = [taskInfo]\n    possible_answers = []\n    thinking, answer = cot_agent(cot_inputs, cot_initial_instruction, 0)\n\n    # Add the answer to the list of possible answers\n    possible_answers.extend([thinking, answer])\n\n    for i in range(N_max):\n        # Reflect on previous attempts and generate another interesting answer\n        cot_inputs.extend([thinking, answer])\n\n        # Generate another interesting answer\n        thinking, answer = cot_agent(cot_inputs, qd_instruction, i + 1)\n        possible_answers.extend([thinking, answer])\n\n    # Make the final decision based on all generated answers\n    thinking, answer = final_decision_agent([taskInfo] + possible_answers, final_decision_instruction)\n    return answer\n",
        "api_calls": 8,
        "structure_label": "Multi-Agent Reasoning",
        "generation": "initial",
        "fitness": "95% Bootstrap Confidence Interval: (42.2%, 59.4%), Median: 50.8%",
        "test_fitness": "95% Bootstrap Confidence Interval: (51.5%, 58.4%), Median: 55.0%"
    },
    {
        "thought": "Similar to Auto-GPT and expert prompting, we can use dynamic control flow in the design to let the agent decide what expert we should use.",
        "name": "Dynamic Assignment of Roles",
        "code": "def forward(self, taskInfo):\n        # Instruction for step-by-step reasoning\n        cot_instruction = \"Please think step by step and then solve the task.\"\n        expert_agents = [LLMAgentBase(['thinking', 'answer'], 'Expert Agent', role=role) for role in ['Math Professor', 'Grade School Teacher', 'Math Enthusiast', 'Helpful Assistant']]\n\n        # Instruction for routing the task to the appropriate expert\n        routing_instruction = \"Given the task, please choose an Expert to answer the question. Choose from: Math Professor, Grade School Teacher, Math Enthusiast.\"\n        routing_agent = LLMAgentBase(['choice'], 'Routing agent')\n\n        # Get the choice of expert to route the task\n        choice = routing_agent([taskInfo], routing_instruction)[0]\n\n        if 'professor' in choice.content.lower():\n            expert_id = 0\n        elif 'teacher' in choice.content.lower():\n            expert_id = 1\n        elif 'enthusiast' in choice.content.lower():\n            expert_id = 2\n        else:\n            expert_id = 3 # Default to helpful assistant\n\n        thinking, answer = expert_agents[expert_id]([taskInfo], cot_instruction)\n        return answer\n",
        "api_calls": 6,
        "structure_label": "Decompositional Reasoning",
        "generation": "initial",
        "fitness": "95% Bootstrap Confidence Interval: (11.7%, 25.0%), Median: 18.0%",
        "test_fitness": "95% Bootstrap Confidence Interval: (11.4%, 16.1%), Median: 13.8%"
    },
    {
        "thought": "**Insights:**\nTo create a more engaging and effective architecture, I propose a structure that not only incorporates iterative refinement but also advances the approach by integrating a targeted feedback mechanism. This design will utilize a primary agent that generates initial outputs, followed by a focused refinement of these outputs based on specific aspects identified during the reasoning process. The goal is to improve accuracy and depth in reasoning while adhering to the specified number of API calls. \n**Overall Idea:**\nThe architecture will consist of an initial phase where the agent generates multiple outputs based on the problem statement. This will be followed by an iterative refinement phase where specific feedback is used to enhance the reasoning process. \n**Implementation:**\n1. Generate diverse initial outputs from a single agent. \n2. Implement a feedback loop that iteratively refines these outputs, focusing on specific elements to improve accuracy. \n3. Ensure the total number of API calls remains within the 'few' category while enhancing result quality.",
        "name": "Refined Iterative Feedback",
        "code": "def forward(self, taskInfo):\n    # Step 1: Initial instruction for generating diverse reasoning outputs\n    initial_instruction = \"Analyze the following problem from multiple perspectives and provide reasoning along with your answers.\"\n    \n    # Create the main agent to explore the problem\n    primary_agent = LLMAgentBase([\"thinking\", \"answers\"], \"Refinement Agent\")\n    \n    # Generate multiple initial outputs in one call\n    outputs = primary_agent([taskInfo], initial_instruction)  # 1 call\n    answers = outputs[1]  # Store the initial answers for iteration\n    \n    # Step 2: Gather previous outputs for refinement\n    previous_answers = answers\n    \n    # Step 3: Iterative refinement loop with targeted feedback\n    N_iterations = 3  # Set number of iterations for refinement\n    for i in range(N_iterations):  # Loop: 3 iterations\n        # Prepare input for the next iteration, focusing on specific improvements\n        feedback_instruction = f\"Refine your reasoning using the previous outputs: {previous_answers}. Focus on improving the total count of pets and explaining your reasoning clearly.\"\n        refined_output = primary_agent([taskInfo, previous_answers], feedback_instruction)  # 1 call\n        previous_answers = refined_output[1]  # Update to the most recent output from the refinement\n    \n    # Step 4: Return the final refined answer after iterations\n    return previous_answers  # Return the most refined answer after all iterations",
        "fitness": "95% Bootstrap Confidence Interval: (75.0%, 88.3%), Median: 82.0%",
        "generation": 80,
        "api_calls": 4,
        "structure_label": "Iterative Refinement",
        "test_fitness": "95% Bootstrap Confidence Interval: (77.0%, 82.5%), Median: 79.8%"
    },
    {
        "thought": "**Insights:**\nTo create a more engaging and effective architecture, I propose a structure that enhances iterative refinement through multiple outputs and feedback loops. This design will utilize a primary agent that generates initial outputs, followed by a feedback mechanism that refines these outputs through multiple iterations. The goal is to improve accuracy and depth in reasoning while adhering to the specified number of API calls. \n**Overall Idea:**\nThe architecture will consist of an initial phase where the agent generates diverse outputs based on the problem statement. This will be followed by several iterative refinement phases, allowing the agent to improve its answers based on previous outputs. This ensures a rich and comprehensive exploration of the problem. \n**Implementation:**\n1. Generate diverse initial outputs from a single agent. \n2. Implement a feedback loop that iteratively refines these outputs, using the previous answers to inform the next round of reasoning. \n3. Ensure that the total number of API calls exceeds five to fall within the many API calls category.",
        "name": "Iterative Refinement and Feedback Architecture",
        "code": "def forward(self, taskInfo):\n    # Step 1: Initial instruction for generating diverse reasoning outputs\n    initial_instruction = \"Analyze the following problem from multiple perspectives and provide reasoning along with your answers.\"\n    \n    # Create the main agent to explore the problem\n    primary_agent = LLMAgentBase([\"thinking\", \"answers\"], \"Diverse Reasoning Agent\")\n    \n    # Generate multiple initial outputs in one call\n    outputs = primary_agent([taskInfo], initial_instruction)  # 1 call\n    answers = outputs[1]  # Store the initial answers for iteration\n    \n    # Step 2: Iterative refinement loop\n    N_iterations = 5  # Set to five iterations for refinement\n    for i in range(N_iterations):  # Loop: 5 iterations\n        # Prepare input for the next iteration, including previous answers\n        feedback_instruction = f\"Refine your reasoning using the previous answers: {answers}. Provide a more accurate total number of pets.\"\n        refined_output = primary_agent([taskInfo, answers], feedback_instruction)  # 1 call\n        answers = refined_output[1]  # Update to the most recent output from the refinement\n    \n    # Step 3: Return the final refined answer after all iterations\n    return answers  # Return the most refined and accurate answer after all iterations",
        "fitness": "95% Bootstrap Confidence Interval: (73.4%, 87.5%), Median: 80.5%",
        "generation": 79,
        "api_calls": 6,
        "structure_label": "Iterative Refinement",
        "test_fitness": "95% Bootstrap Confidence Interval: (75.8%, 81.4%), Median: 78.6%"
    },
    {
        "thought": "**Insights:**\nTo enhance the existing architecture, I propose a design that integrates principle extraction followed by iterative reasoning across multiple outputs. This structure will first distill core principles from the task before generating diverse reasoning paths, ensuring a thorough exploration of potential solutions. The reasoning will then be refined iteratively to achieve a high level of accuracy by incorporating feedback from each iteration. This architecture not only increases the number of API calls but also deepens the reasoning process significantly.\n**Overall Idea:**\nThe design consists of two main phases: first, extracting high-level principles to guide reasoning; second, generating multiple reasoning outputs based on these principles, followed by iterative refinement of those outputs. This will ensure comprehensive problem-solving while adhering to a higher API call count.",
        "name": "Principle Extraction and Iterative Reasoning",
        "code": "def forward(self, taskInfo):\n    # Step 1: Generate high-level principles from the taskInfo\n    principle_instruction = 'Extract the core principles from the following mathematical problem: ' + taskInfo.content\n    principle_agent = LLMAgentBase(['principles'], 'Principle Extractor')\n    principles_output = principle_agent([taskInfo], principle_instruction)  # 1 call\n    principles = principles_output[0].content  # Extract the principles from output\n\n    # Step 2: Generate diverse reasoning outputs based on principles\n    reasoning_instruction = f'Using the principles: {principles}, analyze the following problem from different perspectives and provide reasoning along with your answers.'\n    reasoning_agent = LLMAgentBase(['thinking', 'answers'], 'Diverse Reasoning Agent')\n    reasoning_outputs = reasoning_agent([taskInfo], reasoning_instruction)  # 1 call to generate all outputs at once\n\n    # Step 3: Iterate through the reasoning outputs and refine them\n    refined_answers = []\n    for answer in reasoning_outputs:\n        feedback_instruction = f'Refine your reasoning based on the answer: {answer[1]}.'\n        refined_output = reasoning_agent([taskInfo, answer], feedback_instruction)  # 1 call for refinement\n        refined_answers.append(refined_output[1])  # Collect refined answers\n\n    # Step 4: Return the most refined answer after aggregating multiple outputs\n    final_answer = refined_answers[-1].content  # For simplicity, take the last refined answer\n    return Info('final_answer', 'Principle Extraction Agent', final_answer, 0)",
        "fitness": "95% Bootstrap Confidence Interval: (74.2%, 87.5%), Median: 81.2%",
        "generation": 86,
        "api_calls": 4,
        "structure_label": "Abstraction to Principles Reasoning",
        "test_fitness": "95% Bootstrap Confidence Interval: (74.6%, 80.4%), Median: 77.5%"
    },
    {
        "thought": "**Insights:**\nTo enhance the architecture, I propose a new structure that utilizes an iterative refinement process with a focus on feedback rather than multiple concurrent agents. This design allows for a single agent to generate diverse reasoning outputs in an initial call, followed by a series of refinements based on those outputs. The aim is to maximize the quality of the answer through iterations, while still keeping the total number of API calls within the 'few' limit. \n**Overall Idea:**\nThe new architecture will consist of an initial reasoning phase where a single agent generates multiple outputs, followed by an iterative refinement phase where these outputs are fed back into the agent for further enhancement. This reduces the total number of calls while capturing the benefits of an iterative process. \n**Implementation:**\n1. Use one agent to produce multiple outputs based on the initial task. \n2. Enter a feedback loop that iterates through a set number of refinements based on the previous outputs each time. \n3. At the end of the iterations, return the most refined answer.",
        "name": "Iterative Feedback Refinement",
        "code": "def forward(self, taskInfo):\n    # Step 1: Initial instruction for generating diverse perspectives\n    initial_instruction = \"Please think about this problem from multiple perspectives and provide your reasoning and answers.\"\n    \n    # Create the main agent to explore the problem\n    primary_agent = LLMAgentBase([\"thinking\", \"answers\"], \"Primary Reasoning Agent\")\n    \n    # Generate multiple outputs in one call\n    outputs = primary_agent([taskInfo], initial_instruction)  # 1 call\n    previous_answer = outputs[1]  # Store the initial answer\n    \n    # Step 2: Iterative refinement loop\n    N_iterations = 3  # Set number of iterations for refinement\n    for _ in range(N_iterations):  # Loop: 3 iterations\n        # Prepare input for the next iteration including previous answer\n        feedback_instruction = \"Using the previous output, refine your reasoning and provide an updated answer.\"\n        refined_output = primary_agent([taskInfo, previous_answer], feedback_instruction)  # 1 call\n        previous_answer = refined_output[1]  # Update to the most recent output\n    \n    # Step 3: Return the final answer after all refinements\n    return previous_answer",
        "fitness": "95% Bootstrap Confidence Interval: (71.1%, 85.2%), Median: 78.1%",
        "generation": 29,
        "api_calls": 4,
        "structure_label": "Iterative Refinement",
        "test_fitness": "95% Bootstrap Confidence Interval: (78.5%, 83.9%), Median: 81.2%"
    },
    {
        "thought": "**Insights:**\nTo create a more effective architecture, I propose a design that merges principle extraction with a simplified iterative refinement process. The goal is to focus on obtaining diverse reasoning outputs in a single pass, followed by targeted feedback to enhance a select few outputs rather than multiple iterations. This approach streamlines the reasoning chain while maintaining accuracy. \n**Overall Idea:**\nThe architecture consists of extracting high-level principles, generating reasoning outputs based on those principles, and providing a singular round of focused refinement. This process emphasizes efficiency while still improving performance on the benchmark, ultimately leading to fewer API calls and reduced complexity. \n**Implementation:**\n1. Extract core principles directly from the task.\n2. Generate reasoning outputs based on those principles.\n3. Implement a targeted refinement round for the best outputs, rather than multiple iterations. \n4. Ensure the total number of API calls remains minimal while achieving refined results.",
        "name": "Principle Extraction with Focused Refinement",
        "code": "def forward(self, taskInfo):\n    # Step 1: Generate high-level principles from the taskInfo\n    principle_instruction = 'Extract the core principles from the following mathematical problem: ' + taskInfo.content\n    principle_agent = LLMAgentBase(['principles'], 'Principle Extractor')\n    principles_output = principle_agent([taskInfo], principle_instruction)  # 1 call\n    principles = principles_output[0].content  # Extract the principles from output\n\n    # Step 2: Generate reasoning outputs based on principles\n    reasoning_instruction = f'Using the principles: {principles}, analyze the following problem from different perspectives and provide reasoning along with your answers.'\n    reasoning_agent = LLMAgentBase(['thinking', 'answers'], 'Reasoning Agent')\n    reasoning_outputs = reasoning_agent([taskInfo], reasoning_instruction)  # 1 call to generate all outputs at once\n\n    # Step 3: Focused refinement process using the reasoning output\n    refined_output = reasoning_agent([taskInfo, reasoning_outputs[1]], f'Refine your reasoning based on the answer: {reasoning_outputs[1]}')  # 1 call for refinement\n\n    # Step 4: Return the final refined answer\n    return refined_output[1]  # Return the most refined answer after one targeted refinement.",
        "fitness": "95% Bootstrap Confidence Interval: (68.8%, 83.6%), Median: 76.6%",
        "generation": 89,
        "api_calls": 3,
        "structure_label": "Abstraction to Principles Reasoning",
        "test_fitness": "95% Bootstrap Confidence Interval: (72.8%, 78.8%), Median: 75.8%"
    }
]