[
    {
        "thought": "By encouraging the LLM to think step by step rather than directly outputting an answer, chain-of-thought reasoning enables complex problem-solving through intermediate steps. This practice improves the model's ability to handle tasks that require deeper reasoning and provides insight into its decision-making process.",
        "name": "Chain-of-Thought",
        "code": "def forward(self, taskInfo):\n    # Instruction for the Chain-of-Thought (CoT) approach\n    # It is an important practice that allows the LLM to think step by step before solving the task.\n    cot_instruction = \"Please think step by step and then solve the task.\"\n\n    # Instantiate a new LLM agent specifically for CoT\n    # To allow LLM thinking before answering, we need to set an additional output field 'thinking'.\n    cot_agent = LLMAgentBase(['thinking', 'answer'], 'Chain-of-Thought Agent')\n\n    # Prepare the inputs for the CoT agent\n    # The input should be a list of Info, and the first one is often the taskInfo\n    cot_agent_inputs = [taskInfo]\n\n    # Get the response from the CoT agent\n    thinking, answer = cot_agent(cot_agent_inputs, cot_instruction)\n\n    # Return only the final answer\n    return answer\n",
        "api_calls": 1,
        "structure_label": "Chain-of-Thought Reasoning",
        "generation": "initial",
        "fitness": "95% Bootstrap Confidence Interval: (13.3%, 26.6%), Median: 19.5%"
    },
    {
        "thought": "While an LLM can arrive at the correct answer, its reasoning may vary. By repeatedly asking the same question with high temperature settings, we can generate different reasoning paths. We then combine multiple answers from these Chain-of-Thought (CoT) agents to produce a more accurate final answer through ensembling.",
        "name": "Self-Consistency with Chain-of-Thought",
        "code": "def forward(self, taskInfo):\n    # Instruction for step-by-step reasoning\n    cot_instruction = \"Please think step by step and then solve the task.\"\n    N = 5 # Number of CoT agents\n\n    # Initialize multiple CoT agents with a higher temperature for varied reasoning\n    cot_agents = [LLMAgentBase(['thinking', 'answer'], 'Chain-of-Thought Agent', temperature=0.8) for _ in range(N)]\n\n    # Majority voting function to select the most common answer\n    from collections import Counter\n    def majority_voting(answers):\n        return Counter(answers).most_common(1)[0][0]\n    \n    possible_answers = []\n    for i in range(N):\n        thinking, answer = cot_agents[i]([taskInfo], cot_instruction)\n        possible_answers.append(answer.content)\n\n    # Ensembling the answers from multiple CoT agents\n    answer = majority_voting(possible_answers)\n    return answer  \n",
        "api_calls": 5,
        "structure_label": "Multi-Agent Reasoning",
        "generation": "initial",
        "fitness": "95% Bootstrap Confidence Interval: (10.2%, 23.4%), Median: 16.4%"
    },
    {
        "thought": "To enhance its performance, an LLM can iteratively improve its answer based on feedback. By reflecting on its previous attempts and incorporating feedback, the model can refine its reasoning and provide a more accurate solution.",
        "name": "Self-Refine (Reflexion)",
        "code": "def forward(self, taskInfo):\n    # Instruction for initial reasoning\n    cot_initial_instruction = \"Please think step by step and then solve the task.\"\n\n    # Instruction for reflecting on previous attempts and feedback to improve\n    cot_reflect_instruction = \"Given previous attempts and feedback, carefully consider where you could go wrong in your latest attempt. Using insights from previous attempts, try to solve the task better.\"\n    cot_agent = LLMAgentBase(['thinking', 'answer'], 'Chain-of-Thought Agent')\n\n    # Instruction for providing feedback and correcting the answer\n    critic_instruction = \"Please review the answer above and criticize on where might be wrong. If you are absolutely sure it is correct, output 'True' in 'correct'.\"\n    critic_agent = LLMAgentBase(['feedback', 'correct'], 'Critic Agent')\n    \n    N_max = 5 # Maximum number of attempts\n\n    # Initial attempt\n    cot_inputs = [taskInfo]\n    thinking, answer = cot_agent(cot_inputs, cot_initial_instruction, 0)\n\n    for i in range(N_max):\n        # Get feedback and correct status from the critic\n        feedback, correct = critic_agent([taskInfo, thinking, answer], critic_instruction, i)\n        if correct.content == 'True':\n            break\n            \n        # Add feedback to the inputs for the next iteration\n        cot_inputs.extend([thinking, answer, feedback])\n\n        # Reflect on previous attempts and refine the answer\n        thinking, answer = cot_agent(cot_inputs, cot_reflect_instruction, i + 1)\n    return answer\n",
        "api_calls": 7,
        "structure_label": "Self-Reflection Reasoning",
        "generation": "initial",
        "fitness": "95% Bootstrap Confidence Interval: (10.9%, 24.2%), Median: 17.2%"
    },
    {
        "thought": "By letting different LLMs debate with each other, we can leverage their diverse perspectives to find better solutions for tasks.",
        "name": "LLM Debate",
        "code": "def forward(self, taskInfo):\n    # Instruction for initial reasoning\n    debate_initial_instruction = \"Please think step by step and then solve the task.\"\n\n    # Instruction for debating and updating the solution based on other agents' solutions\n    debate_instruction = \"Given solutions to the problem from other agents, consider their opinions as additional advice. Please think carefully and provide an updated answer.\"\n    \n    # Initialize debate agents with different roles and a moderate temperature for varied reasoning\n    debate_agents = [LLMAgentBase(['thinking', 'answer'], 'Debate Agent', temperature=0.8, role=role) for role in ['Math Professor', 'Grade School Teacher', 'Math Enthusiast']]\n\n    # Instruction for final decision-making based on all debates and solutions\n    final_decision_instruction = \"Given all the above thinking and answers, reason over them carefully and provide a final answer.\"\n    final_decision_agent = LLMAgentBase(['thinking', 'answer'], 'Final Decision Agent', temperature=0.1)\n\n    max_round = 2 # Maximum number of debate rounds\n    all_thinking = [[] for _ in range(max_round)]\n    all_answer = [[] for _ in range(max_round)]\n\n    # Perform debate rounds\n    for r in range(max_round):\n        for i in range(len(debate_agents)):\n            if r == 0:\n                thinking, answer = debate_agents[i]([taskInfo], debate_initial_instruction)\n            else:\n                input_infos = [taskInfo] + [all_thinking[r-1][i]] + all_thinking[r-1][:i] + all_thinking[r-1][i+1:]\n                thinking, answer = debate_agents[i](input_infos, debate_instruction)\n            all_thinking[r].append(thinking)\n            all_answer[r].append(answer)\n    \n    # Make the final decision based on all debate results and solutions\n    thinking, answer = final_decision_agent([taskInfo] + all_thinking[max_round-1] + all_answer[max_round-1], final_decision_instruction)\n    return answer\n",
        "api_calls": 9,
        "structure_label": "Multi-Agent Reasoning",
        "generation": "initial",
        "fitness": "95% Bootstrap Confidence Interval: (44.5%, 61.7%), Median: 53.1%"
    },
    {
        "thought": "Let LLM first think about the principles involved in solving this task which could be helpful. By understanding the underlying principles, the model can better reason through the problem and provide a more accurate solution.",
        "name": "Step-back Abstraction",
        "code": "def forward(self, taskInfo):\n        # Instruction for understanding the principles involved in the task\n        principle_instruction = \"What are the physics, chemistry or biology principles and concepts involved in solving this task? First think step by step. Then list all involved principles and explain them.\"\n        \n        # Instruction for solving the task based on the principles\n        cot_instruction = \"Given the question and the involved principle behind the question, think step by step and then solve the task.\"\n        \n        # Instantiate LLM agents\n        principle_agent = LLMAgentBase(['thinking', 'principle'], 'Principle Agent')\n        cot_agent = LLMAgentBase(['thinking', 'answer'], 'Chain-of-Thought Agent')\n        \n        # Get the principles involved in the task\n        thinking, principle = principle_agent([taskInfo], principle_instruction)\n\n        # Use the principles to solve the task\n        thinking, answer = cot_agent([taskInfo, thinking, principle], cot_instruction)\n        return answer\n",
        "api_calls": 2,
        "structure_label": "Abstraction to Principles Reasoning",
        "generation": "initial",
        "fitness": "95% Bootstrap Confidence Interval: (14.8%, 28.9%), Median: 21.9%"
    },
    {
        "thought": "Similar to Quality-Diversity methods, let LLM generate multiple diverse interesting solutions could help. By encouraging the model to explore different reasoning paths, we can increase the chances of finding the best solution.",
        "name": "Quality-Diversity",
        "code": "def forward(self, taskInfo):\n    # Instruction for initial reasoning\n    cot_initial_instruction = \"Please think step by step and then solve the task.\"\n\n    # Instruction for giving diverse answers\n    qd_instruction = \"Given previous attempts, try to come up with another interesting way to solve the task.\"\n    cot_agent = LLMAgentBase(['thinking', 'answer'], 'Chain-of-Thought Agent')\n\n    # Instruction for final decision-making based on collected reasoning and answers\n    final_decision_instruction = \"Given all the above solutions, reason over them carefully and provide a final answer.\"\n    final_decision_agent = LLMAgentBase(['thinking', 'answer'], 'Final Decision Agent', temperature=0.1)\n    \n    N_max = 3 # Maximum number of attempts\n\n    # Initial attempt\n    cot_inputs = [taskInfo]\n    possible_answers = []\n    thinking, answer = cot_agent(cot_inputs, cot_initial_instruction, 0)\n\n    # Add the answer to the list of possible answers\n    possible_answers.extend([thinking, answer])\n\n    for i in range(N_max):\n        # Reflect on previous attempts and generate another interesting answer\n        cot_inputs.extend([thinking, answer])\n\n        # Generate another interesting answer\n        thinking, answer = cot_agent(cot_inputs, qd_instruction, i + 1)\n        possible_answers.extend([thinking, answer])\n\n    # Make the final decision based on all generated answers\n    thinking, answer = final_decision_agent([taskInfo] + possible_answers, final_decision_instruction)\n    return answer\n",
        "api_calls": 7,
        "structure_label": "Multi-Agent Reasoning",
        "generation": "initial",
        "fitness": "95% Bootstrap Confidence Interval: (46.1%, 63.3%), Median: 54.7%"
    },
    {
        "thought": "Similar to Auto-GPT and expert prompting, we can use dynamic control flow in the design to let the agent decide what expert we should use.",
        "name": "Dynamic Assignment of Roles",
        "code": "def forward(self, taskInfo):\n        # Instruction for step-by-step reasoning\n        cot_instruction = \"Please think step by step and then solve the task.\"\n        expert_agents = [LLMAgentBase(['thinking', 'answer'], 'Expert Agent', role=role) for role in ['Math Professor', 'Grade School Teacher', 'Math Enthusiast', 'Helpful Assistant']]\n\n        # Instruction for routing the task to the appropriate expert\n        routing_instruction = \"Given the task, please choose an Expert to answer the question. Choose from: Math Professor, Grade School Teacher, Math Enthusiast.\"\n        routing_agent = LLMAgentBase(['choice'], 'Routing agent')\n\n        # Get the choice of expert to route the task\n        choice = routing_agent([taskInfo], routing_instruction)[0]\n\n        if 'professor' in choice.content.lower():\n            expert_id = 0\n        elif 'teacher' in choice.content.lower():\n            expert_id = 1\n        elif 'enthusiast' in choice.content.lower():\n            expert_id = 2\n        else:\n            expert_id = 3 # Default to helpful assistant\n\n        thinking, answer = expert_agents[expert_id]([taskInfo], cot_instruction)\n        return answer\n",
        "api_calls": 4,
        "structure_label": "Multi-Agent Reasoning",
        "generation": "initial",
        "fitness": "95% Bootstrap Confidence Interval: (10.9%, 24.2%), Median: 17.2%"
    },
    {
        "thought": "**Insights:**\nTo enhance performance while maintaining clarity in the output, I propose an architecture that clearly separates the principles involved in the task from the final answer. This can still be achieved with a single call to the LLMAgentBase, but the output structure will be refined for better usability.\n\n**Overall Idea:**\nThe agent will ask the LLM to identify the principles involved in solving the task and then to provide a solution based on those principles. The response will clearly delineate the principles from the answer while still keeping it as a single API call.\n\n**Implementation:**\n1. Use a single instruction that prompts the LLM to extract both the principles and the answer in a structured format.\n2. Ensure clarity in the output by specifying that both components should be included but clearly differentiated.\n3. Implement the instructions to minimize ambiguity in the response.",
        "name": "Principles and Solution Separation Agent",
        "code": "def forward(self, taskInfo):\n    # Instruction to think about principles and solve the task clearly\n    combined_instruction = \"What principles and concepts are involved in solving this task? Please think step by step, list the involved principles, and provide the answer, ensuring it's formatted as follows: Principles: [your principles here]. Answer: [your answer here].\"\n    \n    # Instantiate a single LLM agent for combined reasoning\n    combined_agent = LLMAgentBase(['thinking', 'principles', 'answer'], 'Principles and Solution Agent')\n    \n    # Get the response from the combined agent\n    response = combined_agent([taskInfo], combined_instruction)\n    \n    # Ensure the response is structured correctly and clearly returns both components\n    return response[0] if response else Info('answer', 'Error: No valid response.', 0)",
        "fitness": "95% Bootstrap Confidence Interval: (0.0%, 0.0%), Median: 0.0%",
        "generation": 1,
        "api_calls": 1,
        "structure_label": "Abstraction to Principles Reasoning"
    },
    {
        "thought": "**Insights:**\nTo enhance performance and clarity in the output while ensuring minimal API calls, I propose an architecture that combines the extraction of principles and the solution generation into a single, structured response. This will optimize the use of the LLM and provide a clear distinction between the principles involved and the final answer within a single API call.\n\n**Overall Idea:**\nThe agent will prompt the LLM to think through the principles associated with the problem and provide a solution in a unified response format. The instruction will specify that both the principles and the solution must be clearly articulated, which will help maintain clarity while maximizing efficiency.\n\n**Implementation:**\n1. Use a single instruction to extract both the principles and the answer in a structured format, ensuring distinct output for both components.\n2. Implement this with one call to the LLMAgentBase, focusing on clarity and avoiding redundancy in the response structure.",
        "name": "Unified Principles and Solution Agent",
        "code": "def forward(self, taskInfo):\n    # Unified instruction to extract principles and generate answer\n    combined_instruction = \"Identify the principles involved in solving the following problem: {taskInfo}. Please list these principles clearly, followed by your answer. Format your response as: Principles: [list of principles]. Answer: [your answer].\"\n    \n    # Instantiate a single LLM agent for combined reasoning\n    unified_agent = LLMAgentBase(['thinking', 'principles', 'answer'], 'Unified Principles and Solution Agent')\n    \n    # Get the structured response from the unified agent\n    response = unified_agent([taskInfo], combined_instruction)\n    \n    # Ensure the response is structured correctly, return the response directly\n    return response[0] if response else Info('answer', 'Error: No valid response.', 0)",
        "fitness": "95% Bootstrap Confidence Interval: (0.0%, 0.0%), Median: 0.0%",
        "generation": 2,
        "api_calls": 1,
        "structure_label": "Abstraction to Principles Reasoning"
    },
    {
        "thought": "**Insights:**\nTo innovate further, I propose an architecture that uses a single agent to combine principles extraction, answer generation, and internal refinement in a single step. This approach eliminates the need for multiple API calls while ensuring that the output remains clear and structured. By incorporating reflection and refinement directly after generating the initial answer without re-calling the agent, the architecture optimizes performance.\n\n**Overall Idea:**\nThe agent will extract principles and generate an answer in one go, then internally reflect on the principles to refine the answer before returning it. This will maximize the output quality by leveraging the model's inherent ability to self-assess while adhering to the API call limitations.\n\n**Implementation:**\n1. Use a single instruction that prompts the agent to extract principles, generate an answer, then self-reflect to refine that answer in a structured format without the need for a second call.\n2. Ensure the output clearly separates the principles from the answer while performing this in a single API call.",
        "name": "Refined Principles and Self-Reflection Agent",
        "code": "def forward(self, taskInfo):\n    # Unified instruction to extract principles, generate an answer, and reflect on that answer\n    combined_instruction = \"Identify the principles involved in solving the following problem: {taskInfo}. First, list the relevant principles clearly. Then, provide your answer to the problem. Finally, based on the principles you've listed, critique your answer and suggest any improvements. Format your response as follows: Principles: [list of principles]. Answer: [your answer]. Suggestions for Improvement: [your critique].\"\n    \n    # Instantiate a single LLM agent for combined reasoning\n    unified_agent = LLMAgentBase(['thinking', 'principles', 'answer'], 'Refined Principles and Self-Reflection Agent')\n    \n    # Get the structured response from the unified agent\n    response = unified_agent([taskInfo], combined_instruction)\n    \n    # Return the response directly, ensuring correct formatting\n    return response[0] if response else Info('answer', 'Error: No valid response.', 0)",
        "fitness": "95% Bootstrap Confidence Interval: (0.0%, 0.0%), Median: 0.0%",
        "generation": 3,
        "api_calls": 1,
        "structure_label": "Self-Reflection Reasoning"
    },
    {
        "thought": "**Insights:**\nTo address the shortcomings of the previous proposal, I will design an architecture that combines the extraction of principles and the task-solving process within a single API call while also including a reflective mechanism that critiques the identified principles. This approach will ensure that only one instance of LLMAgentBase is used, thus complying with the API call limitations. \n\n**Overall Idea:**\nThe agent will prompt the LLM to list the principles associated with the task while providing an answer and includes a critique of the principles' relevance to the solution in a structured format. This will streamline the process and improve the quality of reasoning. \n\n**Implementation:**\n1. Combine the extraction of principles and the task-solving process into a single instruction.\n2. Ensure that the response format captures principles, the answer, and a reflection in one go.\n3. Use a single LLMAgentBase instance to execute the combined task.",
        "name": "Principles and Reflection Task Solver",
        "code": "def forward(self, taskInfo):\n    # Unified instruction to extract principles and generate an answer with reflection\n    combined_instruction = (\"Identify the principles involved in solving the following problem: {taskInfo}. \"\n    \"List the principles clearly. Then, provide your answer to the problem. \"\n    \"Finally, based on the principles you've listed, critique your answer and suggest any improvements. \"\n    \"Format your response as follows: Principles: [list of principles]. Answer: [your answer]. Suggestions for Improvement: [your critique].\")\n    \n    # Instantiate a single LLM agent for combined reasoning\n    unified_agent = LLMAgentBase(['thinking', 'principles', 'answer', 'reflection'], 'Principles and Reflection Task Solver')\n    \n    # Get the structured response from the unified agent\n    response = unified_agent([taskInfo], combined_instruction)\n    \n    # Validate the response structure and return it if valid\n    if isinstance(response, list) and len(response) > 0:\n        # Check if the expected keys are present in the response\n        expected_keys = ['principles', 'answer', 'suggestions']\n        valid_response = all(key in response[0].content for key in expected_keys)\n        if valid_response:\n            return response[0]\n    return Info('answer', 'Error: No valid response.', 0)",
        "fitness": "95% Bootstrap Confidence Interval: (0.0%, 0.0%), Median: 0.0%",
        "generation": 4,
        "api_calls": 1,
        "structure_label": "Self-Reflection Reasoning"
    },
    {
        "thought": "**Insights:**\nWhile the current implementation is effective, there is room for improvement in how the principles are critiqued. By guiding the model to not only critique its answer but also relate that critique back to the principles identified, we can enhance the overall reasoning process.\n\n**Overall Idea:**\nThe revised agent will still extract principles and generate an answer, but it will place greater emphasis on how those principles directly influence the answer and the critique of it. This will help to ensure that the principles are not just listed but are actively used to improve the answer in a clear manner. \n\n**Implementation:**\n1. **Unified Instruction**: Reinforce the structure of the instruction to ensure that the critique directly references the principles.\n2. **Focused Critique**: Ensure the critique specifically addresses how the principles could lead to a better answer.\n3. **Single LLMAgentBase Instance**: Maintain a single instance for efficiency and compliance with API call limits.",
        "name": "Principles and Insightful Reflection Solver",
        "code": "def forward(self, taskInfo):\n    # Unified instruction to extract principles, generate an answer, and critique based on principles\n    combined_instruction = (\n        \"Identify the principles involved in solving the following problem: {taskInfo}. \"\n        \"List all relevant principles clearly and explain their importance. Then, provide your answer to the problem, explaining how each principle contributes to your answer. \"\n        \"Finally, critique your answer using the principles and suggest specific improvements that could be made. \"\n        \"Format your response as follows: Principles: [list of principles]. Answer: [your answer]. Suggestions for Improvement: [your critique that ties back to the principles].\"\n    )\n    \n    # Instantiate a single LLM agent for combined reasoning\n    unified_agent = LLMAgentBase(['thinking', 'principles', 'answer', 'reflection'], 'Principles and Insightful Reflection Solver')\n    \n    # Get the structured response from the unified agent\n    response = unified_agent([taskInfo], combined_instruction)\n    \n    # Ensure the response is valid and format is correct before returning\n    if isinstance(response, list) and len(response) > 0:\n        return response[0]  # Return the first valid response\n    else:\n        return Info('answer', 'Error: No valid response or incorrect format.', 0)",
        "fitness": "95% Bootstrap Confidence Interval: (0.0%, 0.0%), Median: 0.0%",
        "generation": 5,
        "api_calls": 1,
        "structure_label": "Self-Reflection Reasoning"
    }
]