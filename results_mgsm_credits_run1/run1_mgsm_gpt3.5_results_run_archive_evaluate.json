[
    {
        "thought": "By encouraging the LLM to think step by step rather than directly outputting an answer, chain-of-thought reasoning enables complex problem-solving through intermediate steps. This practice improves the model's ability to handle tasks that require deeper reasoning and provides insight into its decision-making process.",
        "name": "Chain-of-Thought",
        "code": "def forward(self, taskInfo):\n    # Instruction for the Chain-of-Thought (CoT) approach\n    # It is an important practice that allows the LLM to think step by step before solving the task.\n    cot_instruction = \"Please think step by step and then solve the task.\"\n\n    # Instantiate a new LLM agent specifically for CoT\n    # To allow LLM thinking before answering, we need to set an additional output field 'thinking'.\n    cot_agent = LLMAgentBase(['thinking', 'answer'], 'Chain-of-Thought Agent')\n\n    # Prepare the inputs for the CoT agent\n    # The input should be a list of Info, and the first one is often the taskInfo\n    cot_agent_inputs = [taskInfo]\n\n    # Get the response from the CoT agent\n    thinking, answer = cot_agent(cot_agent_inputs, cot_instruction)\n\n    # Return only the final answer\n    return answer\n",
        "api_calls": 1,
        "structure_label": "Chain-of-Thought Reasoning",
        "generation": "initial",
        "fitness": "95% Bootstrap Confidence Interval: (13.3%, 27.3%), Median: 20.3%",
        "test_fitness": "95% Bootstrap Confidence Interval: (12.4%, 17.2%), Median: 14.8%"
    },
    {
        "thought": "While an LLM can arrive at the correct answer, its reasoning may vary. By repeatedly asking the same question with high temperature settings, we can generate different reasoning paths. We then combine multiple answers from these Chain-of-Thought (CoT) agents to produce a more accurate final answer through ensembling.",
        "name": "Self-Consistency with Chain-of-Thought",
        "code": "def forward(self, taskInfo):\n    # Instruction for step-by-step reasoning\n    cot_instruction = \"Please think step by step and then solve the task.\"\n    N = 5 # Number of CoT agents\n\n    # Initialize multiple CoT agents with a higher temperature for varied reasoning\n    cot_agents = [LLMAgentBase(['thinking', 'answer'], 'Chain-of-Thought Agent', temperature=0.8) for _ in range(N)]\n\n    # Majority voting function to select the most common answer\n    from collections import Counter\n    def majority_voting(answers):\n        return Counter(answers).most_common(1)[0][0]\n    \n    possible_answers = []\n    for i in range(N):\n        thinking, answer = cot_agents[i]([taskInfo], cot_instruction)\n        possible_answers.append(answer.content)\n\n    # Ensembling the answers from multiple CoT agents\n    answer = majority_voting(possible_answers)\n    return answer  \n",
        "api_calls": 5,
        "structure_label": "Multi-Agent Reasoning",
        "generation": "initial",
        "fitness": "95% Bootstrap Confidence Interval: (6.2%, 17.2%), Median: 11.7%",
        "test_fitness": "95% Bootstrap Confidence Interval: (10.8%, 15.4%), Median: 13.0%"
    },
    {
        "thought": "To enhance its performance, an LLM can iteratively improve its answer based on feedback. By reflecting on its previous attempts and incorporating feedback, the model can refine its reasoning and provide a more accurate solution.",
        "name": "Self-Refine (Reflexion)",
        "code": "def forward(self, taskInfo):\n    # Instruction for initial reasoning\n    cot_initial_instruction = \"Please think step by step and then solve the task.\"\n\n    # Instruction for reflecting on previous attempts and feedback to improve\n    cot_reflect_instruction = \"Given previous attempts and feedback, carefully consider where you could go wrong in your latest attempt. Using insights from previous attempts, try to solve the task better.\"\n    cot_agent = LLMAgentBase(['thinking', 'answer'], 'Chain-of-Thought Agent')\n\n    # Instruction for providing feedback and correcting the answer\n    critic_instruction = \"Please review the answer above and criticize on where might be wrong. If you are absolutely sure it is correct, output 'True' in 'correct'.\"\n    critic_agent = LLMAgentBase(['feedback', 'correct'], 'Critic Agent')\n    \n    N_max = 5 # Maximum number of attempts\n\n    # Initial attempt\n    cot_inputs = [taskInfo]\n    thinking, answer = cot_agent(cot_inputs, cot_initial_instruction, 0)\n\n    for i in range(N_max):\n        # Get feedback and correct status from the critic\n        feedback, correct = critic_agent([taskInfo, thinking, answer], critic_instruction, i)\n        if correct.content == 'True':\n            break\n            \n        # Add feedback to the inputs for the next iteration\n        cot_inputs.extend([thinking, answer, feedback])\n\n        # Reflect on previous attempts and refine the answer\n        thinking, answer = cot_agent(cot_inputs, cot_reflect_instruction, i + 1)\n    return answer\n",
        "api_calls": 7,
        "structure_label": "Self-Reflection Reasoning",
        "generation": "initial",
        "fitness": "95% Bootstrap Confidence Interval: (11.7%, 25.0%), Median: 18.0%",
        "test_fitness": "95% Bootstrap Confidence Interval: (16.1%, 21.5%), Median: 18.8%"
    },
    {
        "thought": "By letting different LLMs debate with each other, we can leverage their diverse perspectives to find better solutions for tasks.",
        "name": "LLM Debate",
        "code": "def forward(self, taskInfo):\n    # Instruction for initial reasoning\n    debate_initial_instruction = \"Please think step by step and then solve the task.\"\n\n    # Instruction for debating and updating the solution based on other agents' solutions\n    debate_instruction = \"Given solutions to the problem from other agents, consider their opinions as additional advice. Please think carefully and provide an updated answer.\"\n    \n    # Initialize debate agents with different roles and a moderate temperature for varied reasoning\n    debate_agents = [LLMAgentBase(['thinking', 'answer'], 'Debate Agent', temperature=0.8, role=role) for role in ['Math Professor', 'Grade School Teacher', 'Math Enthusiast']]\n\n    # Instruction for final decision-making based on all debates and solutions\n    final_decision_instruction = \"Given all the above thinking and answers, reason over them carefully and provide a final answer.\"\n    final_decision_agent = LLMAgentBase(['thinking', 'answer'], 'Final Decision Agent', temperature=0.1)\n\n    max_round = 2 # Maximum number of debate rounds\n    all_thinking = [[] for _ in range(max_round)]\n    all_answer = [[] for _ in range(max_round)]\n\n    # Perform debate rounds\n    for r in range(max_round):\n        for i in range(len(debate_agents)):\n            if r == 0:\n                thinking, answer = debate_agents[i]([taskInfo], debate_initial_instruction)\n            else:\n                input_infos = [taskInfo] + [all_thinking[r-1][i]] + all_thinking[r-1][:i] + all_thinking[r-1][i+1:]\n                thinking, answer = debate_agents[i](input_infos, debate_instruction)\n            all_thinking[r].append(thinking)\n            all_answer[r].append(answer)\n    \n    # Make the final decision based on all debate results and solutions\n    thinking, answer = final_decision_agent([taskInfo] + all_thinking[max_round-1] + all_answer[max_round-1], final_decision_instruction)\n    return answer\n",
        "api_calls": 9,
        "structure_label": "Multi-Agent Reasoning",
        "generation": "initial",
        "fitness": "95% Bootstrap Confidence Interval: (44.5%, 61.7%), Median: 53.1%",
        "test_fitness": "95% Bootstrap Confidence Interval: (44.5%, 51.5%), Median: 48.0%"
    },
    {
        "thought": "Let LLM first think about the principles involved in solving this task which could be helpful. By understanding the underlying principles, the model can better reason through the problem and provide a more accurate solution.",
        "name": "Step-back Abstraction",
        "code": "def forward(self, taskInfo):\n        # Instruction for understanding the principles involved in the task\n        principle_instruction = \"What are the physics, chemistry or biology principles and concepts involved in solving this task? First think step by step. Then list all involved principles and explain them.\"\n        \n        # Instruction for solving the task based on the principles\n        cot_instruction = \"Given the question and the involved principle behind the question, think step by step and then solve the task.\"\n        \n        # Instantiate LLM agents\n        principle_agent = LLMAgentBase(['thinking', 'principle'], 'Principle Agent')\n        cot_agent = LLMAgentBase(['thinking', 'answer'], 'Chain-of-Thought Agent')\n        \n        # Get the principles involved in the task\n        thinking, principle = principle_agent([taskInfo], principle_instruction)\n\n        # Use the principles to solve the task\n        thinking, answer = cot_agent([taskInfo, thinking, principle], cot_instruction)\n        return answer\n",
        "api_calls": 2,
        "structure_label": "Abstraction to Principles Reasoning",
        "generation": "initial",
        "fitness": "95% Bootstrap Confidence Interval: (17.2%, 32.0%), Median: 24.2%",
        "test_fitness": "95% Bootstrap Confidence Interval: (22.0%, 28.0%), Median: 25.0%"
    },
    {
        "thought": "Similar to Quality-Diversity methods, let LLM generate multiple diverse interesting solutions could help. By encouraging the model to explore different reasoning paths, we can increase the chances of finding the best solution.",
        "name": "Quality-Diversity",
        "code": "def forward(self, taskInfo):\n    # Instruction for initial reasoning\n    cot_initial_instruction = \"Please think step by step and then solve the task.\"\n\n    # Instruction for giving diverse answers\n    qd_instruction = \"Given previous attempts, try to come up with another interesting way to solve the task.\"\n    cot_agent = LLMAgentBase(['thinking', 'answer'], 'Chain-of-Thought Agent')\n\n    # Instruction for final decision-making based on collected reasoning and answers\n    final_decision_instruction = \"Given all the above solutions, reason over them carefully and provide a final answer.\"\n    final_decision_agent = LLMAgentBase(['thinking', 'answer'], 'Final Decision Agent', temperature=0.1)\n    \n    N_max = 3 # Maximum number of attempts\n\n    # Initial attempt\n    cot_inputs = [taskInfo]\n    possible_answers = []\n    thinking, answer = cot_agent(cot_inputs, cot_initial_instruction, 0)\n\n    # Add the answer to the list of possible answers\n    possible_answers.extend([thinking, answer])\n\n    for i in range(N_max):\n        # Reflect on previous attempts and generate another interesting answer\n        cot_inputs.extend([thinking, answer])\n\n        # Generate another interesting answer\n        thinking, answer = cot_agent(cot_inputs, qd_instruction, i + 1)\n        possible_answers.extend([thinking, answer])\n\n    # Make the final decision based on all generated answers\n    thinking, answer = final_decision_agent([taskInfo] + possible_answers, final_decision_instruction)\n    return answer\n",
        "api_calls": 7,
        "structure_label": "Multi-Agent Reasoning",
        "generation": "initial",
        "fitness": "95% Bootstrap Confidence Interval: (43.8%, 60.9%), Median: 52.3%",
        "test_fitness": "95% Bootstrap Confidence Interval: (52.1%, 58.9%), Median: 55.5%"
    },
    {
        "thought": "Similar to Auto-GPT and expert prompting, we can use dynamic control flow in the design to let the agent decide what expert we should use.",
        "name": "Dynamic Assignment of Roles",
        "code": "def forward(self, taskInfo):\n        # Instruction for step-by-step reasoning\n        cot_instruction = \"Please think step by step and then solve the task.\"\n        expert_agents = [LLMAgentBase(['thinking', 'answer'], 'Expert Agent', role=role) for role in ['Math Professor', 'Grade School Teacher', 'Math Enthusiast', 'Helpful Assistant']]\n\n        # Instruction for routing the task to the appropriate expert\n        routing_instruction = \"Given the task, please choose an Expert to answer the question. Choose from: Math Professor, Grade School Teacher, Math Enthusiast.\"\n        routing_agent = LLMAgentBase(['choice'], 'Routing agent')\n\n        # Get the choice of expert to route the task\n        choice = routing_agent([taskInfo], routing_instruction)[0]\n\n        if 'professor' in choice.content.lower():\n            expert_id = 0\n        elif 'teacher' in choice.content.lower():\n            expert_id = 1\n        elif 'enthusiast' in choice.content.lower():\n            expert_id = 2\n        else:\n            expert_id = 3 # Default to helpful assistant\n\n        thinking, answer = expert_agents[expert_id]([taskInfo], cot_instruction)\n        return answer\n",
        "api_calls": 4,
        "structure_label": "Multi-Agent Reasoning",
        "generation": "initial",
        "fitness": "95% Bootstrap Confidence Interval: (7.8%, 19.5%), Median: 13.3%",
        "test_fitness": "95% Bootstrap Confidence Interval: (9.8%, 14.2%), Median: 12.0%"
    },
    {
        "thought": "**Insights:**\nTo enhance the self-reflection process, I propose an architecture that integrates an explicit self-assessment mechanism after evaluating the solution. This will guide the LLM to critique its own logic against predefined criteria such as correctness, completeness, and clarity, ensuring a structured approach to self-reflection. The architecture will also allow for a more iterative refinement process, encouraging deeper reasoning and a more accurate final answer.\n\n**Overall Idea:**\nThe architecture will involve a single agent instructed to solve the task, evaluate its answer based on clear criteria, and reflect on its reasoning quality all in one step. This design minimizes API calls while promoting thorough self-assessment and enhancement of the solution.\n\n**Implementation:**\n1. Define a comprehensive instruction prompting the agent to solve the task step by step, evaluate its solution against specific criteria, and reflect on its reasoning process.\n2. Utilize a single LLMAgentBase instance to handle this entire workflow, ensuring efficient use of resources.\n3. Return the final refined answer based on the self-assessment, providing a fallback in case no answer is generated.",
        "name": "Criteria-Based Self-Assessment Architecture",
        "code": "def forward(self, taskInfo):\n    # Instruction for integrated reasoning and self-assessment\n    reflective_instruction = \"Please solve the task step by step. After generating your answer, evaluate your solution based on the following criteria: correctness, completeness, clarity, and relevance. Reflect on your reasoning quality and provide an improved answer if necessary.\"\n\n    # Instantiate a single LLM agent for integrated reasoning and self-assessment\n    integrated_agent = LLMAgentBase([\"thinking\", \"final_answer\", \"critique\"], \"Criteria-Based Self-Assessment Agent\")\n\n    # Prepare the input for the agent, which includes the taskInfo only\n    inputs = [taskInfo]\n\n    # Get the response from the integrated agent\n    response_infos = integrated_agent(inputs, reflective_instruction)\n\n    # Directly return the answer from the response\n    for info in response_infos:\n        if info.name == 'final_answer':\n            return info\n    # Fallback to ensure an answer structure is maintained\n    return Info('final_answer', 'Criteria-Based Self-Assessment Agent', 'No answer generated.', 0)",
        "fitness": "95% Bootstrap Confidence Interval: (71.9%, 85.9%), Median: 78.9%",
        "generation": 19,
        "api_calls": 1,
        "structure_label": "Self-Reflection Reasoning",
        "test_fitness": "95% Bootstrap Confidence Interval: (69.1%, 75.4%), Median: 72.2%"
    },
    {
        "thought": "**Insights:**\nTo enhance the current architecture while maintaining compliance with the API call restrictions, I propose a design that uses a single critique agent with a comprehensive task instruction that encourages it to provide detailed feedback. Instead of multiple critique agents, a unified approach can allow for collaborative evaluation while reducing the total number of calls. This approach will foster a single, robust critique process that informs the refinement of the answer in a more efficient manner.\n\n**Overall Idea:**\nThe architecture will involve generating an initial answer, followed by a single agent tasked with providing a critique that considers various aspects of the answer. This will simplify the critique process while ensuring the depth and breadth of evaluation through a well-defined instruction set. After receiving feedback, the answer will be refined accordingly, leading to a higher-quality final output with fewer API calls.\n\n**Implementation Steps:**\n1. Generate an initial answer using a reasoning agent based on the task description.\n2. Use a single critique agent with broad instructions to evaluate the initial answer comprehensively.\n3. Refine the initial answer based on the critique received, ensuring clarity and correctness.\n4. Return the final refined answer while adhering to the API call limits.",
        "name": "Collaborative Critique Architecture",
        "code": "def forward(self, taskInfo):\n    # Instruction for generating the initial answer\n    reasoning_instruction = \"Please think step by step and solve the task.\"\n    reasoning_agent = LLMAgentBase([\"thinking\", \"answer\"], \"Reasoning Agent\")\n\n    # Generate the initial answer\n    thinking, initial_answer = reasoning_agent([taskInfo], reasoning_instruction)\n\n    # Instruction for a comprehensive critique and refinement\n    critique_refine_instruction = \"Critique the following answer and provide specific feedback. Then, refine the answer based on your critique.\"\n    critique_agent = LLMAgentBase([\"feedback\", \"refined_answer\"], \"Critique and Refinement Agent\")\n\n    # Get feedback and refine the initial answer in one call\n    thinking, refined_answer = critique_agent([taskInfo, thinking, initial_answer], critique_refine_instruction)\n    return refined_answer",
        "fitness": "95% Bootstrap Confidence Interval: (68.8%, 83.6%), Median: 76.6%",
        "generation": 28,
        "api_calls": 2,
        "structure_label": "Self-Reflection Reasoning",
        "test_fitness": "95% Bootstrap Confidence Interval: (73.8%, 79.6%), Median: 76.8%"
    },
    {
        "thought": "**Insights:**\nTo enhance the dynamics of expert selection and incorporate feedback mechanisms, I will create an architecture that not only routes tasks to the appropriate expert agent but also allows for iterative refinement of answers based on feedback from a critique agent. This will encourage deeper reasoning and better solutions. The new architecture will consist of a routing mechanism based on expert roles and a reflective process for answer improvement.\n\n**Overall Idea:**\nThe new architecture will first determine which expert to engage based on the task's context and then generate an initial answer. After that, it will involve a feedback loop where a critique agent evaluates the initial answer and provides insights for enhancement. This reflective step aims to refine the answer iteratively, potentially leading to more accurate solutions.\n\n**Implementation Steps:**\n1. Use a routing agent to determine which expert agent should solve the task based on the task description.\n2. Generate an initial answer using the selected expert agent.\n3. Implement a critique agent that provides feedback on the answer.\n4. Allow for a reflective iteration where the answer is improved based on critique feedback.\n5. Ensure the total number of API calls is within allowed limits while maintaining clarity in the reasoning process.",
        "name": "Expert Routing with Reflective Feedback",
        "code": "def forward(self, taskInfo):\n    # Instruction for routing to the appropriate expert\n    routing_instruction = \"Given the task, please choose an Expert to answer the question. Choose from: Math Professor, Grade School Teacher, Math Enthusiast.\"\n    routing_agent = LLMAgentBase(['choice'], 'Routing agent')\n\n    # Get the choice of expert to route the task\n    choice = routing_agent([taskInfo], routing_instruction)[0]\n\n    # Map the choice to an expert ID\n    expert_map = {'professor': 0, 'teacher': 1, 'enthusiast': 2}\n    expert_id = expert_map.get(choice.content.lower(), 3)  # Default to helpful assistant if not found\n\n    # Instruction for step-by-step reasoning\n    cot_instruction = \"Please think step by step and then solve the task.\"\n    expert_agents = [LLMAgentBase(['thinking', 'answer'], 'Expert Agent', role=role) for role in ['Math Professor', 'Grade School Teacher', 'Math Enthusiast', 'Helpful Assistant']]\n\n    # Get initial answer from the selected expert agent\n    thinking, initial_answer = expert_agents[expert_id]([taskInfo], cot_instruction)\n\n    # Instruction for feedback on the initial answer\n    critique_instruction = \"Evaluate the answer provided above. What are its strengths and weaknesses, and how could it be improved?\"\n    critique_agent = LLMAgentBase(['feedback'], 'Critique Agent')\n\n    # Get feedback on the initial answer\n    feedback = critique_agent([taskInfo, thinking, initial_answer], critique_instruction)[0]\n\n    # Instruction for refining the answer based on feedback\n    refine_instruction = \"Using the feedback provided, refine your initial answer.\"\n    refined_answer_agent = LLMAgentBase(['thinking', 'refined_answer'], 'Refinement Agent')\n\n    # Refine the original answer based on critique feedback\n    thinking, refined_answer = refined_answer_agent([taskInfo, thinking, initial_answer, feedback], refine_instruction)\n    return refined_answer",
        "fitness": "95% Bootstrap Confidence Interval: (68.0%, 82.8%), Median: 75.8%",
        "generation": 2,
        "api_calls": 6,
        "structure_label": "Self-Reflection Reasoning",
        "test_fitness": "95% Bootstrap Confidence Interval: (77.5%, 83.0%), Median: 80.2%"
    }
]