{
    "Linear Chain-of-Thought,0": {
        "thought": "By encouraging the LLM to think step by step rather than directly outputting an answer, chain-of-thought reasoning enables complex problem-solving through intermediate steps. This practice improves the model's ability to handle tasks that require deeper reasoning and provides insight into its decision-making process.",
        "name": "Chain-of-Thought",
        "code": "def forward(self, taskInfo):\n    # Instruction for the Chain-of-Thought (CoT) approach\n    # It is an important practice that allows the LLM to think step by step before solving the task.\n    cot_instruction = \"Please think step by step and then solve the task.\"\n\n    # Instantiate a new LLM agent specifically for CoT\n    # To allow LLM thinking before answering, we need to set an additional output field 'thinking'.\n    cot_agent = LLMAgentBase(['thinking', 'answer'], 'Chain-of-Thought Agent')\n\n    # Prepare the inputs for the CoT agent\n    # The input should be a list of Info, and the first one is often the taskInfo\n    cot_agent_inputs = [taskInfo]\n\n    # Get the response from the CoT agent\n    thinking, answer = cot_agent(cot_agent_inputs, cot_instruction)\n\n    # Return only the final answer\n    return answer\n",
        "api_calls": 1,
        "structure_label": "Linear Chain-of-Thought",
        "generation": "initial",
        "fitness": "95% Bootstrap Confidence Interval: (68.8%, 83.6%), Median: 76.6%"
    },
    "Linear Chain-of-Thought,1": null,
    "Iterative Refinement,0": {
        "thought": "**Insights:**\nThe existing architecture could benefit from a more streamlined approach that focuses on refining a single answer iteratively rather than relying on multiple agents for sub-tasks. This can enhance the reasoning process while reducing the complexity of managing multiple outputs.\n**Overall Idea:**\nUtilize a single agent that refines its answers in iterative steps. The agent will provide an initial answer and then improve upon it in subsequent iterations until a satisfactory solution is achieved. This not only reduces API calls but also allows the model to dynamically adjust its reasoning based on immediate feedback from its previous outputs.\n**Implementation:**\n1. Define instructions for the initial answer generation and subsequent refinement.\n2. Introduce a loop for the refinement process while ensuring the number of API calls remains low.\n3. Return the final refined answer after a specified number of iterations.",
        "name": "Iterative Refinement Agent",
        "code": "def forward(self, taskInfo):\n    # Instruction for generating and refining the answer\n    instruction = \"Given the following task, first provide an answer. Then refine it based on your previous output.\"\n\n    # Create the LLM agent instance\n    iterative_agent = LLMAgentBase(['thinking', 'answer'], 'Iterative Refinement Agent')\n\n    # Initialize the first response\n    initial_answer = iterative_agent([taskInfo], instruction)  # 1 call\n\n    # Refinement loop - limit to 2 iterations\n    for _ in range(2):  # 2 iterations x 1 call = 2 calls\n        initial_answer = iterative_agent([taskInfo, initial_answer], instruction)  # 1 call for refinement\n\n    return initial_answer  # Return the final refined answer",
        "fitness": "95% Bootstrap Confidence Interval: (68.8%, 83.6%), Median: 76.6%",
        "generation": 4,
        "api_calls": 4,
        "structure_label": "Iterative Refinement"
    },
    "Iterative Refinement,1": {
        "thought": "To enhance its performance, an LLM can iteratively improve its answer based on feedback. By reflecting on its previous attempts and incorporating feedback, the model can refine its reasoning and provide a more accurate solution.",
        "name": "Self-Refine (Reflexion)",
        "code": "def forward(self, taskInfo):\n    # Instruction for initial reasoning\n    cot_initial_instruction = \"Please think step by step and then solve the task.\"\n\n    # Instruction for reflecting on previous attempts and feedback to improve\n    cot_reflect_instruction = \"Given previous attempts and feedback, carefully consider where you could go wrong in your latest attempt. Using insights from previous attempts, try to solve the task better.\"\n    cot_agent = LLMAgentBase(['thinking', 'answer'], 'Chain-of-Thought Agent')\n\n    # Instruction for providing feedback and correcting the answer\n    critic_instruction = \"Please review the answer above and criticize on where might be wrong. If you are absolutely sure it is correct, output 'True' in 'correct'.\"\n    critic_agent = LLMAgentBase(['feedback', 'correct'], 'Critic Agent')\n    \n    N_max = 5 # Maximum number of attempts\n\n    # Initial attempt\n    cot_inputs = [taskInfo]\n    thinking, answer = cot_agent(cot_inputs, cot_initial_instruction, 0)\n\n    for i in range(N_max):\n        # Get feedback and correct status from the critic\n        feedback, correct = critic_agent([taskInfo, thinking, answer], critic_instruction, i)\n        if correct.content == 'True':\n            break\n            \n        # Add feedback to the inputs for the next iteration\n        cot_inputs.extend([thinking, answer, feedback])\n\n        # Reflect on previous attempts and refine the answer\n        thinking, answer = cot_agent(cot_inputs, cot_reflect_instruction, i + 1)\n    return answer\n",
        "api_calls": 10,
        "structure_label": "Iterative Refinement",
        "generation": "initial",
        "fitness": "95% Bootstrap Confidence Interval: (64.8%, 80.5%), Median: 72.7%"
    },
    "Tree-of-Thought,0": null,
    "Tree-of-Thought,1": null,
    "Decompositional Reasoning,0": {
        "thought": "**Insights:**\nTo maximize efficiency while still addressing the requirements of a complex reasoning task, I propose a Decompositional Reasoning architecture that utilizes a single agent to process the task in distinct sub-steps. This architecture will prioritize clarity and reduce API calls while maintaining the ability to handle complex reasoning. \n\n**Overall Idea:**\nThe proposed design will break the task into independent sub-tasks, which the single agent will handle in sequence. The output of each sub-task will be stored and used to inform the next step, culminating in a final answer that integrates insights from each stage. This method should improve reasoning accuracy while adhering to the API call constraints. \n\n**Implementation:**\n1. Define the overall task instruction to guide the agent. \n2. Within the forward method, implement a single call to the agent, which will handle the entire reasoning process by breaking the task into manageable parts internally, thus consolidating the steps previously handled by multiple agents.\n3. Ensure the reasoning is clear and each step logically leads to the next, culminating in the final answer.",
        "name": "Decompositional Reasoning Agent",
        "code": "def forward(self, taskInfo):\n    # Instruction to break down the task into smaller sub-tasks and provide reasoning step by step.\n    decomposition_instruction = \"Break down the task into smaller sub-tasks and provide a comprehensive answer, including principles, approaches, and final application.\"\n    \n    # Instantiate a single LLM agent for the overall task\n    agent = LLMAgentBase(['thinking', 'answer'], 'Decompositional Reasoning Agent')\n    \n    # Call the agent with the taskInfo and the decomposition instruction\n    answer = agent([taskInfo], decomposition_instruction)  # 1 call\n    \n    # Return the final answer from the agent\n    return answer",
        "fitness": "95% Bootstrap Confidence Interval: (67.2%, 82.0%), Median: 75.0%",
        "generation": 2,
        "api_calls": 1,
        "structure_label": "Decompositional Reasoning"
    },
    "Decompositional Reasoning,1": null,
    "Multi-Agent Reasoning,0": {
        "thought": "While an LLM can arrive at the correct answer, its reasoning may vary. By repeatedly asking the same question with high temperature settings, we can generate different reasoning paths. We then combine multiple answers from these Chain-of-Thought (CoT) agents to produce a more accurate final answer through ensembling.",
        "name": "Self-Consistency with Chain-of-Thought",
        "code": "def forward(self, taskInfo):\n    # Instruction for step-by-step reasoning\n    cot_instruction = \"Please think step by step and then solve the task.\"\n    N = 5 # Number of CoT agents\n\n    # Initialize multiple CoT agents with a higher temperature for varied reasoning\n    cot_agents = [LLMAgentBase(['thinking', 'answer'], 'Chain-of-Thought Agent', temperature=0.8) for _ in range(N)]\n\n    # Majority voting function to select the most common answer\n    from collections import Counter\n    def majority_voting(answers):\n        return Counter(answers).most_common(1)[0][0]\n    \n    possible_answers = []\n    for i in range(N):\n        thinking, answer = cot_agents[i]([taskInfo], cot_instruction)\n        possible_answers.append(answer.content)\n\n    # Ensembling the answers from multiple CoT agents\n    answer = majority_voting(possible_answers)\n    return answer  \n",
        "api_calls": 5,
        "structure_label": "Multi-Agent Reasoning",
        "generation": "initial",
        "fitness": "95% Bootstrap Confidence Interval: (68.8%, 83.6%), Median: 76.6%"
    },
    "Multi-Agent Reasoning,1": {
        "thought": "By letting different LLMs debate with each other, we can leverage their diverse perspectives to find better solutions for tasks.",
        "name": "LLM Debate",
        "code": "def forward(self, taskInfo):\n    # Instruction for initial reasoning\n    debate_initial_instruction = \"Please think step by step and then solve the task.\"\n\n    # Instruction for debating and updating the solution based on other agents' solutions\n    debate_instruction = \"Given solutions to the problem from other agents, consider their opinions as additional advice. Please think carefully and provide an updated answer.\"\n    \n    # Initialize debate agents with different roles and a moderate temperature for varied reasoning\n    debate_agents = [LLMAgentBase(['thinking', 'answer'], 'Debate Agent', temperature=0.8, role=role) for role in ['Biology Expert', 'Physics Expert', 'Chemistry Expert', 'Science Generalist']]\n\n    # Instruction for final decision-making based on all debates and solutions\n    final_decision_instruction = \"Given all the above thinking and answers, reason over them carefully and provide a final answer.\"\n    final_decision_agent = LLMAgentBase(['thinking', 'answer'], 'Final Decision Agent', temperature=0.1)\n\n    max_round = 2 # Maximum number of debate rounds\n    all_thinking = [[] for _ in range(max_round)]\n    all_answer = [[] for _ in range(max_round)]\n\n    # Perform debate rounds\n    for r in range(max_round):\n        for i in range(len(debate_agents)):\n            if r == 0:\n                thinking, answer = debate_agents[i]([taskInfo], debate_initial_instruction)\n            else:\n                input_infos = [taskInfo] + [all_thinking[r-1][i]] + all_thinking[r-1][:i] + all_thinking[r-1][i+1:]\n                thinking, answer = debate_agents[i](input_infos, debate_instruction)\n            all_thinking[r].append(thinking)\n            all_answer[r].append(answer)\n    \n    # Make the final decision based on all debate results and solutions\n    thinking, answer = final_decision_agent([taskInfo] + all_thinking[max_round-1] + all_answer[max_round-1], final_decision_instruction)\n    return answer\n",
        "api_calls": 16,
        "structure_label": "Multi-Agent Reasoning",
        "generation": "initial",
        "fitness": "95% Bootstrap Confidence Interval: (68.0%, 82.8%), Median: 75.8%"
    },
    "Abstraction to Principles Reasoning,0": {
        "thought": "Let LLM first think about the principles involved in solving this task which could be helpful. By understanding the underlying principles, the model can better reason through the problem and provide a more accurate solution.",
        "name": "Step-back Abstraction",
        "code": "def forward(self, taskInfo):\n        # Instruction for understanding the principles involved in the task\n        principle_instruction = \"What are the physics, chemistry or biology principles and concepts involved in solving this task? First think step by step. Then list all involved principles and explain them.\"\n        \n        # Instruction for solving the task based on the principles\n        cot_instruction = \"Given the question and the involved principle behind the question, think step by step and then solve the task.\"\n        \n        # Instantiate LLM agents\n        principle_agent = LLMAgentBase(['thinking', 'principle'], 'Principle Agent')\n        cot_agent = LLMAgentBase(['thinking', 'answer'], 'Chain-of-Thought Agent')\n        \n        # Get the principles involved in the task\n        thinking, principle = principle_agent([taskInfo], principle_instruction)\n\n        # Use the principles to solve the task\n        thinking, answer = cot_agent([taskInfo, thinking, principle], cot_instruction)\n        return answer\n",
        "api_calls": 4,
        "structure_label": "Abstraction to Principles Reasoning",
        "generation": "initial",
        "fitness": "95% Bootstrap Confidence Interval: (71.9%, 85.9%), Median: 78.9%"
    },
    "Abstraction to Principles Reasoning,1": {
        "thought": "**Insights:**\nThe architecture can be enhanced by incorporating high-level principles derived from the task information to guide the iterative refinement process. By generating an initial answer and then refining it while referencing these principles, the agent can improve its reasoning quality significantly. This dual approach can make the architecture more robust and innovative.\n\n**Overall Idea:**\nThe new architecture will consist of two phases: one for generating an initial response and another for refining that response using both the previous output and derived principles from the task. The principles will be generated from the task context and serve as guidance during the refinement iterations.\n\n**Implementation:**\n1. Start by extracting high-level principles from the task information using a dedicated agent.\n2. Utilize these principles to inform the iterative refinement of the initial answer.\n3. Implement voting or consensus mechanisms to finalize the refined answer, emphasizing the use of principles to bolster the answer quality.",
        "name": "Principle-Guided Iterative Refinement",
        "code": "def forward(self, taskInfo):\n    # Phase 1: Principle Extraction\n    principle_instruction = \"Analyze the task and extract high-level principles.\"\n    N_principles = 3  # Number of agents for principle extraction\n    principle_agents = [LLMAgentBase(['thinking', 'principle'], 'Principle Agent') for _ in range(N_principles)]\n\n    principles = []\n    for agent in principle_agents:\n        thinking, principle = agent([taskInfo], principle_instruction)  # 1 call per agent, total: 3 calls\n        principles.append(principle.content)\n\n    # Aggregate principles (assume simple concatenation)\n    aggregated_principles = ' '.join(principles)\n\n    # Phase 2: Answer Generation\n    instruction = \"Given the following task, first provide an answer. Then refine it based on your previous output and the guiding principles.\"\n    iterative_agent = LLMAgentBase(['thinking', 'answer'], 'Iterative Refinement Agent')\n\n    # Initial response\n    initial_answer = iterative_agent([taskInfo], instruction)  # 1 call for initial answer\n\n    # Refinement loop - limit to 2 iterations for deeper refinement\n    for _ in range(2):  # 2 iterations x 1 call = 2 calls\n        refined_answer = iterative_agent([taskInfo, initial_answer, aggregated_principles], instruction)  # 1 call for refinement\n        initial_answer = refined_answer  # Update for the next iteration\n\n    return initial_answer  # Return the final refined answer",
        "fitness": "95% Bootstrap Confidence Interval: (68.0%, 82.8%), Median: 75.8%",
        "generation": 5,
        "api_calls": 8,
        "structure_label": "Abstraction to Principles Reasoning"
    }
}