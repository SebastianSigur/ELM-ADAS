{
    "Linear Chain-of-Thought,0": {
        "thought": "By encouraging the LLM to think step by step rather than directly outputting an answer, chain-of-thought reasoning enables complex problem-solving through intermediate steps. This practice improves the model's ability to handle tasks that require deeper reasoning and provides insight into its decision-making process.",
        "name": "Chain-of-Thought",
        "code": "def forward(self, taskInfo):\n    # Instruction for the Chain-of-Thought (CoT) approach\n    # It is an important practice that allows the LLM to think step by step before solving the task.\n    cot_instruction = \"Please think step by step and then solve the task.\"\n\n    # Instantiate a new LLM agent specifically for CoT\n    # To allow LLM thinking before answering, we need to set an additional output field 'thinking'.\n    cot_agent = LLMAgentBase(['thinking', 'answer'], 'Chain-of-Thought Agent')\n\n    # Prepare the inputs for the CoT agent\n    # The input should be a list of Info, and the first one is often the taskInfo\n    cot_agent_inputs = [taskInfo]\n\n    # Get the response from the CoT agent\n    thinking, answer = cot_agent(cot_agent_inputs, cot_instruction)\n\n    # Return only the final answer\n    return answer\n",
        "api_calls": 1,
        "structure_label": "Linear Chain-of-Thought",
        "generation": "initial",
        "fitness": "95% Bootstrap Confidence Interval: (5.5%, 15.6%), Median: 10.2%"
    },
    "Linear Chain-of-Thought,1": null,
    "Iterative Refinement,0": {
        "thought": "To enhance its performance, an LLM can iteratively improve its answer based on feedback. By reflecting on its previous attempts and incorporating feedback, the model can refine its reasoning and provide a more accurate solution.",
        "name": "Self-Refine (Reflexion)",
        "code": "def forward(self, taskInfo):\n    # Instruction for initial reasoning\n    cot_initial_instruction = \"Please think step by step and then solve the task.\"\n\n    # Instruction for reflecting on previous attempts and feedback to improve\n    cot_reflect_instruction = \"Given previous attempts and feedback, carefully consider where you could go wrong in your latest attempt. Using insights from previous attempts, try to solve the task better.\"\n    cot_agent = LLMAgentBase(['thinking', 'answer'], 'Chain-of-Thought Agent')\n\n    # Instruction for providing feedback and correcting the answer\n    critic_instruction = \"Please review the answer above and criticize on where might be wrong. If you are absolutely sure it is correct, output 'True' in 'correct'.\"\n    critic_agent = LLMAgentBase(['feedback', 'correct'], 'Critic Agent')\n    \n    N_max = 5 # Maximum number of attempts\n\n    # Initial attempt\n    cot_inputs = [taskInfo]\n    thinking, answer = cot_agent(cot_inputs, cot_initial_instruction, 0)\n\n    for i in range(N_max):\n        # Get feedback and correct status from the critic\n        feedback, correct = critic_agent([taskInfo, thinking, answer], critic_instruction, i)\n        if correct.content == 'True':\n            break\n            \n        # Add feedback to the inputs for the next iteration\n        cot_inputs.extend([thinking, answer, feedback])\n\n        # Reflect on previous attempts and refine the answer\n        thinking, answer = cot_agent(cot_inputs, cot_reflect_instruction, i + 1)\n    return answer\n",
        "api_calls": 10,
        "structure_label": "Iterative Refinement",
        "generation": "initial",
        "fitness": "95% Bootstrap Confidence Interval: (11.7%, 25.0%), Median: 18.0%"
    },
    "Iterative Refinement,1": null,
    "Tree-of-Thought,0": null,
    "Tree-of-Thought,1": null,
    "Decompositional Reasoning,0": {
        "thought": "Similar to Auto-GPT and expert prompting, we can use dynamic control flow in the design to let the agent decide what expert we should use.",
        "name": "Dynamic Assignment of Roles",
        "code": "def forward(self, taskInfo):\n        # Instruction for step-by-step reasoning\n        cot_instruction = \"Please think step by step and then solve the task.\"\n        expert_agents = [LLMAgentBase(['thinking', 'answer'], 'Expert Agent', role=role) for role in ['Math Professor', 'Grade School Teacher', 'Math Enthusiast', 'Helpful Assistant']]\n\n        # Instruction for routing the task to the appropriate expert\n        routing_instruction = \"Given the task, please choose an Expert to answer the question. Choose from: Math Professor, Grade School Teacher, Math Enthusiast.\"\n        routing_agent = LLMAgentBase(['choice'], 'Routing agent')\n\n        # Get the choice of expert to route the task\n        choice = routing_agent([taskInfo], routing_instruction)[0]\n\n        if 'professor' in choice.content.lower():\n            expert_id = 0\n        elif 'teacher' in choice.content.lower():\n            expert_id = 1\n        elif 'enthusiast' in choice.content.lower():\n            expert_id = 2\n        else:\n            expert_id = 3 # Default to helpful assistant\n\n        thinking, answer = expert_agents[expert_id]([taskInfo], cot_instruction)\n        return answer\n",
        "api_calls": 6,
        "structure_label": "Decompositional Reasoning",
        "generation": "initial",
        "fitness": "95% Bootstrap Confidence Interval: (7.8%, 19.5%), Median: 13.3%"
    },
    "Decompositional Reasoning,1": null,
    "Multi-Agent Reasoning,0": {
        "thought": "**Insights:**\nTo enhance the architecture's efficiency and innovative approach to multi-agent reasoning, I propose a structure that utilizes a single reasoning agent that can generate multiple diverse solutions based on variations in parameters (like temperature). A consensus agent will then synthesize these outputs into a final answer, ensuring fewer API calls while maintaining robustness in reasoning.\n**Overall Idea:**\nThe architecture will consist of one reasoning agent generating distinct answers based on slight variations in its setup (temperature, role, etc.). A single consensus agent will aggregate these outputs. This will allow for a broader exploration of possible answers while keeping API calls to a minimum.",
        "name": "Diverse Solution Generation Agent",
        "code": "def forward(self, taskInfo):\n    # Step 1: Instruction for the reasoning agent to provide diverse answers\n    initial_instruction = \"Please think creatively and provide your answer.\"\n    \n    # Step 2: Create a reasoning agent\n    agent = LLMAgentBase(['thinking', 'answer'], \"Diverse Reasoning Agent\")\n    answers = []\n\n    # Step 3: Generate answers with slight parameter variations\n    for i in range(2):  # Generate two distinct answers\n        temperature = 0.7 + 0.1 * i  # Slightly different temperature\n        thinking, answer = agent([taskInfo], initial_instruction + f' (Variation {i + 1})')\n        answers.append(answer)\n\n    # Step 4: Final decision based on the diverse answers\n    final_input = [taskInfo] + answers\n    final_agent = LLMAgentBase(['thinking', 'final_answer'], \"Final Consensus Agent\")\n    thinking, final_answer = final_agent(final_input, \"Provide a final consensus answer based on the diverse solutions.\")\n    return final_answer",
        "fitness": "95% Bootstrap Confidence Interval: (53.1%, 70.3%), Median: 61.7%",
        "generation": 14,
        "api_calls": 4,
        "structure_label": "Multi-Agent Reasoning"
    },
    "Multi-Agent Reasoning,1": {
        "thought": "**Insights:**\nThe architecture should enhance the process of exploration and synthesis by allowing agents to branch out based on feedback dynamically. Instead of simply critiquing each other's outputs, agents will also generate alternative answers based on the critiques received, promoting a more enriched set of reasoning paths. This approach should improve overall performance while adhering to the API call regulations.\n**Overall Idea:**\nCreate a dynamic reasoning architecture where multiple agents generate a range of possible answers. These answers will be critiqued, and based on the critiques, agents will generate alternative paths or refine their solutions. This iterative process will not only enhance the diversity of thought but also ensure that multiple perspectives are considered before arriving at a final solution.\n**Implementation:**\n1. Create multiple agents that independently generate answers using the same initial instruction.\n2. Each agent will provide feedback not only for other agents\u2019 outputs but will also use that feedback to generate an alternative answer.\n3. After a set number of iterations, synthesize all outputs into a final answer using a dedicated synthesis agent.",
        "name": "Dynamic Exploration and Synthesis Agent",
        "code": "def forward(self, taskInfo):\n    # Initial instruction for each reasoning agent to generate diverse answers\n    initial_instruction = \"Think step by step and provide your answer.\"\n    # Feedback instruction for agents to critique and improve answers\n    feedback_instruction = \"Review the provided answer, critique it, and suggest improvements.\"\n    # Number of agents to explore different paths\n    num_agents = 5\n\n    # Step 1: Generate initial answers from multiple agents\n    agents = [LLMAgentBase([\"thinking\", \"answer\"], f\"Reasoning Agent {i+1}\") for i in range(num_agents)]\n    answers = []\n\n    for agent in agents:\n        thinking, answer = agent([taskInfo], initial_instruction)\n        answers.append(answer)\n\n    # Step 2: Collect feedback and generate alternative answers in a single pass\n    alternative_answers = []\n    for i in range(num_agents):\n        feedbacks = []\n        for j in range(num_agents):\n            if i != j:\n                feedback = agents[j]([taskInfo, answers[i]], feedback_instruction)\n                feedbacks.append(feedback)\n        # Generate an alternative answer based on feedback and initial answer\n        combined_input = [taskInfo] + feedbacks + [answers[i]]\n        thinking, alternative_answer = agents[i](combined_input, initial_instruction)\n        alternative_answers.append(alternative_answer)\n\n    # Step 3: Synthesize final answers from all refined answers\n    final_instruction = \"Given all the refined answers, provide a final conclusion.\"\n    final_agent = LLMAgentBase([\"thinking\", \"final_answer\"], \"Final Synthesizer\")\n    final_thinking, final_answer = final_agent(alternative_answers, final_instruction)\n\n    return final_answer",
        "fitness": "95% Bootstrap Confidence Interval: (9.4%, 21.9%), Median: 15.6%",
        "generation": 7,
        "api_calls": 35,
        "structure_label": "Multi-Agent Reasoning"
    },
    "Abstraction to Principles Reasoning,0": {
        "thought": "Let LLM first think about the principles involved in solving this task which could be helpful. By understanding the underlying principles, the model can better reason through the problem and provide a more accurate solution.",
        "name": "Step-back Abstraction",
        "code": "def forward(self, taskInfo):\n        # Instruction for understanding the principles involved in the task\n        principle_instruction = \"What are the physics, chemistry or biology principles and concepts involved in solving this task? First think step by step. Then list all involved principles and explain them.\"\n        \n        # Instruction for solving the task based on the principles\n        cot_instruction = \"Given the question and the involved principle behind the question, think step by step and then solve the task.\"\n        \n        # Instantiate LLM agents\n        principle_agent = LLMAgentBase(['thinking', 'principle'], 'Principle Agent')\n        cot_agent = LLMAgentBase(['thinking', 'answer'], 'Chain-of-Thought Agent')\n        \n        # Get the principles involved in the task\n        thinking, principle = principle_agent([taskInfo], principle_instruction)\n\n        # Use the principles to solve the task\n        thinking, answer = cot_agent([taskInfo, thinking, principle], cot_instruction)\n        return answer\n",
        "api_calls": 4,
        "structure_label": "Abstraction to Principles Reasoning",
        "generation": "initial",
        "fitness": "95% Bootstrap Confidence Interval: (16.4%, 31.2%), Median: 23.4%"
    },
    "Abstraction to Principles Reasoning,1": null
}