{
    "Linear Chain-of-Thought,0": {
        "thought": "By encouraging the LLM to think step by step rather than directly outputting an answer, chain-of-thought reasoning enables complex problem-solving through intermediate steps. This practice improves the model's ability to handle tasks that require deeper reasoning and provides insight into its decision-making process.",
        "name": "Chain-of-Thought",
        "code": "def forward(self, taskInfo):\n    # Instruction for the Chain-of-Thought (CoT) approach\n    # It is an important practice that allows the LLM to think step by step before solving the task.\n    cot_instruction = \"Please think step by step and then solve the task.\"\n\n    # Instantiate a new LLM agent specifically for CoT\n    # To allow LLM thinking before answering, we need to set an additional output field 'thinking'.\n    cot_agent = LLMAgentBase(['thinking', 'answer'], 'Chain-of-Thought Agent')\n\n    # Prepare the inputs for the CoT agent\n    # The input should be a list of Info, and the first one is often the taskInfo\n    cot_agent_inputs = [taskInfo]\n\n    # Get the response from the CoT agent\n    thinking, answer = cot_agent(cot_agent_inputs, cot_instruction)\n\n    # Return only the final answer\n    return answer\n",
        "api_calls": 1,
        "structure_label": "Linear Chain-of-Thought",
        "generation": "initial",
        "fitness": "95% Bootstrap Confidence Interval: (5.5%, 15.6%), Median: 10.2%"
    },
    "Linear Chain-of-Thought,1": null,
    "Iterative Refinement,0": {
        "thought": "To enhance its performance, an LLM can iteratively improve its answer based on feedback. By reflecting on its previous attempts and incorporating feedback, the model can refine its reasoning and provide a more accurate solution.",
        "name": "Self-Refine (Reflexion)",
        "code": "def forward(self, taskInfo):\n    # Instruction for initial reasoning\n    cot_initial_instruction = \"Please think step by step and then solve the task.\"\n\n    # Instruction for reflecting on previous attempts and feedback to improve\n    cot_reflect_instruction = \"Given previous attempts and feedback, carefully consider where you could go wrong in your latest attempt. Using insights from previous attempts, try to solve the task better.\"\n    cot_agent = LLMAgentBase(['thinking', 'answer'], 'Chain-of-Thought Agent')\n\n    # Instruction for providing feedback and correcting the answer\n    critic_instruction = \"Please review the answer above and criticize on where might be wrong. If you are absolutely sure it is correct, output 'True' in 'correct'.\"\n    critic_agent = LLMAgentBase(['feedback', 'correct'], 'Critic Agent')\n    \n    N_max = 5 # Maximum number of attempts\n\n    # Initial attempt\n    cot_inputs = [taskInfo]\n    thinking, answer = cot_agent(cot_inputs, cot_initial_instruction, 0)\n\n    for i in range(N_max):\n        # Get feedback and correct status from the critic\n        feedback, correct = critic_agent([taskInfo, thinking, answer], critic_instruction, i)\n        if correct.content == 'True':\n            break\n            \n        # Add feedback to the inputs for the next iteration\n        cot_inputs.extend([thinking, answer, feedback])\n\n        # Reflect on previous attempts and refine the answer\n        thinking, answer = cot_agent(cot_inputs, cot_reflect_instruction, i + 1)\n    return answer\n",
        "api_calls": 10,
        "structure_label": "Iterative Refinement",
        "generation": "initial",
        "fitness": "95% Bootstrap Confidence Interval: (11.7%, 25.0%), Median: 18.0%"
    },
    "Iterative Refinement,1": null,
    "Tree-of-Thought,0": null,
    "Tree-of-Thought,1": null,
    "Decompositional Reasoning,0": {
        "thought": "Similar to Auto-GPT and expert prompting, we can use dynamic control flow in the design to let the agent decide what expert we should use.",
        "name": "Dynamic Assignment of Roles",
        "code": "def forward(self, taskInfo):\n        # Instruction for step-by-step reasoning\n        cot_instruction = \"Please think step by step and then solve the task.\"\n        expert_agents = [LLMAgentBase(['thinking', 'answer'], 'Expert Agent', role=role) for role in ['Math Professor', 'Grade School Teacher', 'Math Enthusiast', 'Helpful Assistant']]\n\n        # Instruction for routing the task to the appropriate expert\n        routing_instruction = \"Given the task, please choose an Expert to answer the question. Choose from: Math Professor, Grade School Teacher, Math Enthusiast.\"\n        routing_agent = LLMAgentBase(['choice'], 'Routing agent')\n\n        # Get the choice of expert to route the task\n        choice = routing_agent([taskInfo], routing_instruction)[0]\n\n        if 'professor' in choice.content.lower():\n            expert_id = 0\n        elif 'teacher' in choice.content.lower():\n            expert_id = 1\n        elif 'enthusiast' in choice.content.lower():\n            expert_id = 2\n        else:\n            expert_id = 3 # Default to helpful assistant\n\n        thinking, answer = expert_agents[expert_id]([taskInfo], cot_instruction)\n        return answer\n",
        "api_calls": 6,
        "structure_label": "Decompositional Reasoning",
        "generation": "initial",
        "fitness": "95% Bootstrap Confidence Interval: (7.8%, 19.5%), Median: 13.3%"
    },
    "Decompositional Reasoning,1": null,
    "Multi-Agent Reasoning,0": {
        "thought": "**Insights:**\nTo enhance the agent's reasoning capabilities while still focusing on extracting principles, I propose a multi-layered architecture that employs multiple agents to generate diverse solutions. This approach will involve an initial principle extraction phase, followed by the generation of multiple answers through different reasoning agents. The final step will involve a consensus mechanism to synthesize the best answer from the diverse outputs. This architecture not only adheres to the required structure of abstraction to principles but also maximizes the number of API calls while promoting exploration of different reasoning paths.\n**Overall Idea:**\nThe agent will first extract relevant principles from the task, then generate solutions through multiple agents, and finally synthesize these solutions to arrive at a more robust final answer. This will utilize the strengths of diverse reasoning paths and collective decision-making.\n**Implementation:**\n1. Extract principles using an initial agent.\n2. Create multiple agents that will independently generate their answers based on the principles extracted.\n3. Implement a final agent to collect and synthesize the results from the various agents into a consensus answer.",
        "name": "Diverse Reasoning Synthesis Agent",
        "code": "def forward(self, taskInfo):\n    # Step 1: Instruction to extract principles\n    principle_instruction = \"Identify the principles involved in this task.\"\n    principle_agent = LLMAgentBase([\"thinking\", \"principles\"], \"Principle Extraction Agent\")\n    principles = principle_agent([taskInfo], principle_instruction)[1]\n\n    # Step 2: Create a single agent for generating diverse answers\n    diverse_answer_instruction_template = \"Using the identified principles, provide a step-by-step solution to the problem. Variation {variation_num}\"\n    answer_agent = LLMAgentBase([\"thinking\", \"answer\"], \"Diverse Reasoning Agent\")\n    answers = []\n\n    # Generate multiple answers with slight variations\n    for i in range(3):  # Number of variations\n        diverse_answer_instruction = diverse_answer_instruction_template.format(variation_num=i + 1)\n        answer = answer_agent([taskInfo, principles], diverse_answer_instruction)[1]\n        answers.append(answer)\n\n    # Step 3: Consensus mechanism to synthesize the answers\n    consensus_instruction = \"Given the diverse answers, provide a final consensus answer.\"\n    consensus_agent = LLMAgentBase([\"thinking\", \"final_answer\"], \"Consensus Agent\")\n    final_answer = consensus_agent([taskInfo] + answers, consensus_instruction)[1]\n\n    return final_answer",
        "fitness": "95% Bootstrap Confidence Interval: (54.7%, 71.9%), Median: 63.3%",
        "generation": 19,
        "api_calls": 6,
        "structure_label": "Multi-Agent Reasoning"
    },
    "Multi-Agent Reasoning,1": {
        "thought": "**Insights:**\nThe architecture should enhance the process of exploration and synthesis by allowing agents to branch out based on feedback dynamically. Instead of simply critiquing each other's outputs, agents will also generate alternative answers based on the critiques received, promoting a more enriched set of reasoning paths. This approach should improve overall performance while adhering to the API call regulations.\n**Overall Idea:**\nCreate a dynamic reasoning architecture where multiple agents generate a range of possible answers. These answers will be critiqued, and based on the critiques, agents will generate alternative paths or refine their solutions. This iterative process will not only enhance the diversity of thought but also ensure that multiple perspectives are considered before arriving at a final solution.\n**Implementation:**\n1. Create multiple agents that independently generate answers using the same initial instruction.\n2. Each agent will provide feedback not only for other agents\u2019 outputs but will also use that feedback to generate an alternative answer.\n3. After a set number of iterations, synthesize all outputs into a final answer using a dedicated synthesis agent.",
        "name": "Dynamic Exploration and Synthesis Agent",
        "code": "def forward(self, taskInfo):\n    # Initial instruction for each reasoning agent to generate diverse answers\n    initial_instruction = \"Think step by step and provide your answer.\"\n    # Feedback instruction for agents to critique and improve answers\n    feedback_instruction = \"Review the provided answer, critique it, and suggest improvements.\"\n    # Number of agents to explore different paths\n    num_agents = 5\n\n    # Step 1: Generate initial answers from multiple agents\n    agents = [LLMAgentBase([\"thinking\", \"answer\"], f\"Reasoning Agent {i+1}\") for i in range(num_agents)]\n    answers = []\n\n    for agent in agents:\n        thinking, answer = agent([taskInfo], initial_instruction)\n        answers.append(answer)\n\n    # Step 2: Collect feedback and generate alternative answers in a single pass\n    alternative_answers = []\n    for i in range(num_agents):\n        feedbacks = []\n        for j in range(num_agents):\n            if i != j:\n                feedback = agents[j]([taskInfo, answers[i]], feedback_instruction)\n                feedbacks.append(feedback)\n        # Generate an alternative answer based on feedback and initial answer\n        combined_input = [taskInfo] + feedbacks + [answers[i]]\n        thinking, alternative_answer = agents[i](combined_input, initial_instruction)\n        alternative_answers.append(alternative_answer)\n\n    # Step 3: Synthesize final answers from all refined answers\n    final_instruction = \"Given all the refined answers, provide a final conclusion.\"\n    final_agent = LLMAgentBase([\"thinking\", \"final_answer\"], \"Final Synthesizer\")\n    final_thinking, final_answer = final_agent(alternative_answers, final_instruction)\n\n    return final_answer",
        "fitness": "95% Bootstrap Confidence Interval: (9.4%, 21.9%), Median: 15.6%",
        "generation": 7,
        "api_calls": 35,
        "structure_label": "Multi-Agent Reasoning"
    },
    "Abstraction to Principles Reasoning,0": {
        "thought": "Let LLM first think about the principles involved in solving this task which could be helpful. By understanding the underlying principles, the model can better reason through the problem and provide a more accurate solution.",
        "name": "Step-back Abstraction",
        "code": "def forward(self, taskInfo):\n        # Instruction for understanding the principles involved in the task\n        principle_instruction = \"What are the physics, chemistry or biology principles and concepts involved in solving this task? First think step by step. Then list all involved principles and explain them.\"\n        \n        # Instruction for solving the task based on the principles\n        cot_instruction = \"Given the question and the involved principle behind the question, think step by step and then solve the task.\"\n        \n        # Instantiate LLM agents\n        principle_agent = LLMAgentBase(['thinking', 'principle'], 'Principle Agent')\n        cot_agent = LLMAgentBase(['thinking', 'answer'], 'Chain-of-Thought Agent')\n        \n        # Get the principles involved in the task\n        thinking, principle = principle_agent([taskInfo], principle_instruction)\n\n        # Use the principles to solve the task\n        thinking, answer = cot_agent([taskInfo, thinking, principle], cot_instruction)\n        return answer\n",
        "api_calls": 4,
        "structure_label": "Abstraction to Principles Reasoning",
        "generation": "initial",
        "fitness": "95% Bootstrap Confidence Interval: (16.4%, 31.2%), Median: 23.4%"
    },
    "Abstraction to Principles Reasoning,1": null
}