{
    "Iterative Refinement,0": null,
    "Iterative Refinement,1": {
        "thought": "To enhance its performance, an LLM can iteratively improve its answer based on feedback. By reflecting on its previous attempts and incorporating feedback, the model can refine its reasoning and provide a more accurate solution.",
        "name": "Self-Refine (Reflexion)",
        "code": "def forward(self, taskInfo):\n    # Instruction for initial reasoning\n    cot_initial_instruction = \"Please think step by step and then solve the task.\"\n\n    # Instruction for reflecting on previous attempts and feedback to improve\n    cot_reflect_instruction = \"Given previous attempts and feedback, carefully consider where you could go wrong in your latest attempt. Using insights from previous attempts, try to solve the task better.\"\n    cot_agent = LLMAgentBase(['thinking', 'answer'], 'Chain-of-Thought Agent')\n\n    # Instruction for providing feedback and correcting the answer\n    critic_instruction = \"Please review the answer above and criticize on where might be wrong. If you are absolutely sure it is correct, output 'True' in 'correct'.\"\n    critic_agent = LLMAgentBase(['feedback', 'correct'], 'Critic Agent')\n    \n    N_max = 5 # Maximum number of attempts\n\n    # Initial attempt\n    cot_inputs = [taskInfo]\n    thinking, answer = cot_agent(cot_inputs, cot_initial_instruction, 0)\n\n    for i in range(N_max):\n        # Get feedback and correct status from the critic\n        feedback, correct = critic_agent([taskInfo, thinking, answer], critic_instruction, i)\n        if correct.content == 'True':\n            break\n            \n        # Add feedback to the inputs for the next iteration\n        cot_inputs.extend([thinking, answer, feedback])\n\n        # Reflect on previous attempts and refine the answer\n        thinking, answer = cot_agent(cot_inputs, cot_reflect_instruction, i + 1)\n    return answer\n",
        "api_calls": 7,
        "structure_label": "Iterative Refinement",
        "generation": "initial",
        "fitness": "95% Bootstrap Confidence Interval: (11.7%, 25.0%), Median: 18.0%"
    },
    "Decompositional Reasoning,0": {
        "thought": "Similar to Auto-GPT and expert prompting, we can use dynamic control flow in the design to let the agent decide what expert we should use.",
        "name": "Dynamic Assignment of Roles",
        "code": "def forward(self, taskInfo):\n        # Instruction for step-by-step reasoning\n        cot_instruction = \"Please think step by step and then solve the task.\"\n        expert_agents = [LLMAgentBase(['thinking', 'answer'], 'Expert Agent', role=role) for role in ['Math Professor', 'Grade School Teacher', 'Math Enthusiast', 'Helpful Assistant']]\n\n        # Instruction for routing the task to the appropriate expert\n        routing_instruction = \"Given the task, please choose an Expert to answer the question. Choose from: Math Professor, Grade School Teacher, Math Enthusiast.\"\n        routing_agent = LLMAgentBase(['choice'], 'Routing agent')\n\n        # Get the choice of expert to route the task\n        choice = routing_agent([taskInfo], routing_instruction)[0]\n\n        if 'professor' in choice.content.lower():\n            expert_id = 0\n        elif 'teacher' in choice.content.lower():\n            expert_id = 1\n        elif 'enthusiast' in choice.content.lower():\n            expert_id = 2\n        else:\n            expert_id = 3 # Default to helpful assistant\n\n        thinking, answer = expert_agents[expert_id]([taskInfo], cot_instruction)\n        return answer\n",
        "api_calls": 4,
        "structure_label": "Decompositional Reasoning",
        "generation": "initial",
        "fitness": "95% Bootstrap Confidence Interval: (7.8%, 19.5%), Median: 13.3%"
    },
    "Decompositional Reasoning,1": null,
    "Abstraction to Principles Reasoning,0": {
        "thought": "Let LLM first think about the principles involved in solving this task which could be helpful. By understanding the underlying principles, the model can better reason through the problem and provide a more accurate solution.",
        "name": "Step-back Abstraction",
        "code": "def forward(self, taskInfo):\n        # Instruction for understanding the principles involved in the task\n        principle_instruction = \"What are the physics, chemistry or biology principles and concepts involved in solving this task? First think step by step. Then list all involved principles and explain them.\"\n        \n        # Instruction for solving the task based on the principles\n        cot_instruction = \"Given the question and the involved principle behind the question, think step by step and then solve the task.\"\n        \n        # Instantiate LLM agents\n        principle_agent = LLMAgentBase(['thinking', 'principle'], 'Principle Agent')\n        cot_agent = LLMAgentBase(['thinking', 'answer'], 'Chain-of-Thought Agent')\n        \n        # Get the principles involved in the task\n        thinking, principle = principle_agent([taskInfo], principle_instruction)\n\n        # Use the principles to solve the task\n        thinking, answer = cot_agent([taskInfo, thinking, principle], cot_instruction)\n        return answer\n",
        "api_calls": 2,
        "structure_label": "Abstraction to Principles Reasoning",
        "generation": "initial",
        "fitness": "95% Bootstrap Confidence Interval: (17.2%, 32.0%), Median: 24.2%"
    },
    "Abstraction to Principles Reasoning,1": null,
    "Multi-Agent Reasoning,0": null,
    "Multi-Agent Reasoning,1": {
        "thought": "**Insights:**\nTo further enhance the performance of multi-agent reasoning, I propose an architecture that allows for collaborative generation and critique of solutions while minimizing redundant calls. This architecture will retain multiple reasoning agents but will streamline the feedback process by allowing a single feedback agent to critique all the generated solutions at once.\n\n**Overall Idea:**\nThis architecture will utilize several reasoning agents to generate independent solutions. Afterward, a single feedback agent will evaluate the solutions collectively, providing a structured critique that is integrated into the final decision-making phase. This ensures that we remain within the API call limit while maximizing the effectiveness of the critique process.",
        "name": "Collaborative Critique Architecture",
        "code": "def forward(self, taskInfo):\n    # Instruction for independent reasoning\n    initial_instruction = \"Please think step by step and generate a potential solution to the task.\"\n\n    # Number of reasoning agents\n    N = 4  # Keeping the number of reasoning agents for diverse solutions\n    agents = [LLMAgentBase(['thinking', 'answer'], 'Reasoning Agent', temperature=0.8) for _ in range(N)]\n    possible_answers = []\n\n    # First Phase: Independent reasoning\n    for agent in agents:\n        thinking, answer = agent([taskInfo], initial_instruction)\n        possible_answers.append(answer)\n\n    # Phase: Collecting critiques from a single feedback agent\n    critique_instruction = \"Critique the following solutions and provide suggestions for improvement.\"\n    feedback_agent = LLMAgentBase(['thinking', 'feedback'], 'Feedback Agent')\n    feedback_thinking, feedback_response = feedback_agent([taskInfo] + possible_answers, critique_instruction)\n\n    # Final Decision Making based on critiques and original solutions\n    final_decision_instruction = \"Evaluate the original solutions and their critiques, then select the best answer.\"\n    final_decision_agent = LLMAgentBase(['thinking', 'final_answer'], 'Final Decision Agent', temperature=0.1)\n    final_thinking, final_answer = final_decision_agent([taskInfo] + possible_answers + [feedback_response], final_decision_instruction)\n    return final_answer",
        "fitness": "95% Bootstrap Confidence Interval: (50.0%, 67.2%), Median: 58.6%",
        "generation": 4,
        "api_calls": 7,
        "structure_label": "Multi-Agent Reasoning"
    },
    "Linear Chain-of-Thought,0": {
        "thought": "**Insights:**\nTo enhance the efficiency of the reasoning process while adhering to the constraints of API calls, I propose an architecture that combines the generation of solutions and critique into a single phase. The idea is to have a single agent generate a solution and simultaneously state its reasoning and critiques, allowing for a more diverse output while remaining within the API call limits.\n\n**Overall Idea:**\nThis proposed architecture will utilize a single agent to think through potential solutions and critique them in the same step, thus streamlining the process and reducing the number of API calls. This agent will be guided to think step-by-step about the problem requirements and mathematical relationships. \n\n**Implementation:**\n1. Set a clear instruction that guides the LLM to generate potential solutions while also critiquing its own reasoning in the same call.\n2. Use a single LLMAgentBase instance to encapsulate the entire reasoning and critique process, thus keeping the structure linear and minimizing the number of API calls.",
        "name": "Integrated Reasoning and Critique",
        "code": "def forward(self, taskInfo):\n    # Instruction for integrated reasoning and self-critique\n    integrated_instruction = \"Please think step by step, generate a potential solution to the task, and provide a critique of your reasoning, ensuring clarity and correctness in your answer.\"\n\n    # Instantiate a new LLM agent for integrated reasoning\n    integrated_agent = LLMAgentBase(['thinking', 'answer'], 'Integrated Reasoning Agent')\n\n    # Prepare the inputs for the agent\n    integrated_agent_inputs = [taskInfo]\n\n    # Get the response from the integrated agent\n    thinking, answer = integrated_agent(integrated_agent_inputs, integrated_instruction)\n\n    # Return the final answer directly\n    return answer",
        "fitness": "95% Bootstrap Confidence Interval: (14.8%, 28.9%), Median: 21.9%",
        "generation": 5,
        "api_calls": 1,
        "structure_label": "Linear Chain-of-Thought"
    },
    "Linear Chain-of-Thought,1": null
}